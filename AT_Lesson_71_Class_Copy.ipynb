{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalinis07/APT_Class_Copy_Links/blob/MASTER/AT_Lesson_71_Class_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN_oEsXfM_LV"
      },
      "source": [
        "# Lesson 71: Car Price Prediction - Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFjOAv8nAhzQ"
      },
      "source": [
        "### Teacher-Student Activities\n",
        "\n",
        "After rebuilding the linear regression model, in this class, we will find out the final features affecting the prices of cars and evaluate the model on all the important metrics such as coefficient of determination, MAE, MSE, RMSE and homoscedasticity. You will also get to learn basics of logarithm and another evaluation metric called MSLE.\n",
        "\n",
        "Let's go through the activities covered in the previous class and begin this class from **Activity 1: Final Features** section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rqO31eQB5O-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-uhY3ryaj4N"
      },
      "source": [
        "### Problem Statement\n",
        "\n",
        "Build a linear regression model to predict prices of cars based on its technical specifications such as car manufacturer, its engine capacity, fuel efficiency, body-type etc.\n",
        "\n",
        "**Dataset Description:**\n",
        "\n",
        "The dataset contains 205 rows and 26 columns. Each column represents an attribute of a car as described in the table below.\n",
        "\n",
        "|Sr No.|Attribute|Attribute Information|\n",
        "|-|-|-|\n",
        "|1|Car_ID|Unique id of each car (Integer)|\n",
        "|2|Symboling|Assigned insurance risk rating; a value of +3 indicates that the car is risky; -3 suggests that it is probably a safe car (Categorical)|\n",
        "|3|carCompany|Name of car company (Categorical)|\n",
        "|4|fueltype| fuel-type i.e. petrol or diesel (Categorical)|\n",
        "|5|aspiration|Aspiration used in a car (Categorical)|\n",
        "|6|doornumber|Number of doors in a car (Categorical)|\n",
        "|7|carbody|Body-type of a car (Categorical)|\n",
        "|8|drivewheel|Type of drive wheel (Categorical)|\n",
        "|9|enginelocation|Location of car engine (Categorical)|\n",
        "|10|wheelbase|Wheelbase of car (Numeric)|\n",
        "|11|carlength|Length of car (Numeric)|\n",
        "|12|carwidth|Width of car (Numeric)|\n",
        "|13|carheight|Height of car (Numeric)|\n",
        "|14|curbweight|The weight of a car without occupants or baggage (Numeric)|\n",
        "|15|enginetype|Type of engine (Categorical)|\n",
        "|16|cylindernumber|Number of cylinders placed in the car engine (Categorical)||17|enginesize|Capacity of an engine (Numeric)|\n",
        "|18|fuelsystem|Fuel system of a car (Categorical)|\n",
        "|19|boreratio|Bore ratio of car (Numeric)|\n",
        "|20|stroke|Stroke or volume inside the engine (Numeric)|\n",
        "|21|compressionratio|Compression ratio of an engine (Numeric)|\n",
        "|22|horsepower|Power output of an engine (Numeric)|\n",
        "|23|peakrpm|Peak revolutions per minute (Numeric)|\n",
        "|24|citympg|Mileage in city (Numeric)|\n",
        "|25|highwaympg|Mileage on highway (Numeric)|\n",
        "|26|price(Dependent variable)|Price of a car (Numeric)|\n",
        "\n",
        "This data set consists of three types of entities:\n",
        "\n",
        "- the specification of an auto in terms of various characteristics,\n",
        "\n",
        "- its assigned insurance risk rating,\n",
        "\n",
        "- its normalised losses in use as compared to other cars.\n",
        "\n",
        "The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process **symboling**. A value of $+3$ indicates that the auto is risky, $-3$ that it is probably pretty safe.\n",
        "\n",
        "The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality etc.), and represents the average loss per car per year.\n",
        "\n",
        "**Note:** Several of the attributes in the database could be used as a \"class\" attribute.\n",
        "\n",
        "**Dataset source:** https://archive.ics.uci.edu/ml/datasets/Automobile\n",
        "\n",
        "\n",
        "The above dataset consists of data taken from 1985 Ward's Automotive Yearbook. Here's the list of original sources of the data:\n",
        "\n",
        "1. 1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook.\n",
        "\n",
        "2. Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038\n",
        "\n",
        "3. Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZEA9P6hDG28"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVeYBHNDHh8"
      },
      "source": [
        "#### Importing Modules & Reading Data\n",
        "\n",
        "https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/car-prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f2emlnJM56A"
      },
      "source": [
        "# Import the modules, read the dataset and create a Pandas DataFrame.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Read the dataset\n",
        "cars_df = pd.read_csv(\"https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/car-prices.csv\")\n",
        "\n",
        "\n",
        "# Extract the name of the manufactures from the car names and display the first 25 cars to verify whether names are extracted successfully.\n",
        "car_companies = pd.Series([car.split(\" \")[0] for car in cars_df['CarName']], index = cars_df.index)\n",
        "\n",
        "# Create a new column named 'car_company'. It should store the company names of a the cars.\n",
        "cars_df['car_company'] = car_companies\n",
        "\n",
        "# Replace the misspelled 'car_company' names with their correct names.\n",
        "# volkswagen\n",
        "cars_df.loc[(cars_df['car_company'] == \"vw\") | (cars_df['car_company'] == \"vokswagen\"), 'car_company'] = 'volkswagen'\n",
        "\n",
        "# porsche\n",
        "cars_df.loc[cars_df['car_company'] == \"porcshce\", 'car_company'] = 'porsche'\n",
        "\n",
        "# toyota\n",
        "cars_df.loc[cars_df['car_company'] == \"toyouta\", 'car_company'] = 'toyota'\n",
        "\n",
        "# nissan\n",
        "cars_df.loc[cars_df['car_company'] == \"Nissan\", 'car_company'] = 'nissan'\n",
        "\n",
        "# mazda\n",
        "cars_df.loc[cars_df['car_company'] == \"maxda\", 'car_company'] = 'mazda'\n",
        "\n",
        "# Drop 'CarName' column from the 'cars_df' DataFrame.\n",
        "cars_df.drop(columns= ['CarName'], axis = 1, inplace = True)\n",
        "\n",
        "# Extract all the numeric (float and int type) columns from the dataset.\n",
        "cars_numeric_df = cars_df.select_dtypes(include = ['int64', 'float64'])\n",
        "\n",
        "# Drop the 'car_ID' column from the 'cars_numeric_df' DataFrame.\n",
        "cars_numeric_df.drop(columns = ['car_ID'], axis = 1, inplace = True)\n",
        "\n",
        "# Map the values of the 'doornumber' and 'cylindernumber' columns to their corresponding numeric values.\n",
        "words_dict = {\"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\": 6, \"eight\": 8, \"twelve\": 12}\n",
        "def num_map(series):\n",
        "    return series.map(words_dict)\n",
        "\n",
        "# Applying the function to the two columns\n",
        "cars_df[['cylindernumber', 'doornumber']] = cars_df[['cylindernumber', 'doornumber']].apply(num_map, axis = 1)\n",
        "\n",
        "# Create dummy variables for the 'carbody' columns.\n",
        "car_body_dummies = pd.get_dummies(cars_df['carbody'], dtype = int)\n",
        "\n",
        "# Create dummy variables for the 'carbody' columns with 1 column less.\n",
        "car_body_new_dummies = pd.get_dummies(cars_df['carbody'], drop_first = True, dtype = int)\n",
        "\n",
        "# Create a DataFrame containing all the non-numeric type features.\n",
        "cars_categorical_df = cars_df.select_dtypes(include = ['object'])\n",
        "\n",
        "#Get dummy variables for all the categorical type columns using the dummy coding process.\n",
        "cars_dummies_df = pd.get_dummies(cars_categorical_df, drop_first = True, dtype = int)\n",
        "\n",
        "#  Drop the categorical type columns from the 'cars_df' DataFrame.\n",
        "cars_df.drop(list(cars_categorical_df.columns), axis = 1, inplace = True)\n",
        "\n",
        "# Concatenate the 'cars_df' and 'cars_dummies_df' DataFrames.\n",
        "cars_df = pd.concat([cars_df, cars_dummies_df], axis = 1)\n",
        "\n",
        "#  Drop the 'car_ID' column\n",
        "cars_df.drop('car_ID', axis = 1, inplace = True)\n",
        "\n",
        "# Split the 'cars_df' Dataframe into the train and test sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(cars_df, test_size = 0.3, random_state = 42)\n",
        "\n",
        "# Create separate data-frames for the feature and target variables for both the train and test sets.\n",
        "features = list(cars_df.columns)\n",
        "features.remove('price')\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df['price']\n",
        "X_test = test_df[features]\n",
        "y_test = test_df['price']\n",
        "\n",
        "# Normalise only the numeric columns.\n",
        "def standard_norm(series):\n",
        "  series_mean = series.mean()\n",
        "  series_std = series.std()\n",
        "  new_series = (series - series_mean) / series_std\n",
        "  return new_series\n",
        "\n",
        "X_train[X_train.columns[:16]] = X_train[X_train.columns[:16]].apply(standard_norm, axis = 0)\n",
        "X_test[X_test.columns[:16]] = X_test[X_test.columns[:16]].apply(standard_norm, axis = 0)\n",
        "\n",
        "# Highly correlated features\n",
        "major_features = {}\n",
        "for f in features:\n",
        "  corr_coef = np.corrcoef(cars_df['price'], cars_df[f])[0, 1]\n",
        "  if (corr_coef >= 0.5) or (corr_coef <= -0.5):\n",
        "    major_features[f] = corr_coef\n",
        "\n",
        "print(\"Number of features moderately to highly correlated with price =\", len(major_features), \"\\n\")\n",
        "major_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI8qitijEfI3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3n9WzqImdBs"
      },
      "source": [
        "#### Recursive Feature Elimination (RFE)\n",
        "\n",
        "Select the best 10 features out of the highly correlated features that you just stored in the dictionary above.\n",
        "\n",
        "To use RFE, you need to\n",
        "\n",
        "1. Import the `RFE` class from the `sklearn.feature_selection` module.\n",
        "\n",
        "2. Create an object of a `LinearRegression` class. Let's name it `skl_lin_reg`.\n",
        "\n",
        "3. Create an object of the `RFE` class. Let's name it `rfe1`. The constructor of the `RFE` object takes the object of `LinearRegression` class (i.e. `skl_lin_reg`) and the number of features to be selected using RFE as inputs.\n",
        "\n",
        "4. Call the `fit()` function on the `RFE` object with the train set (separate sets for independent variables and dependent variable) as input to the function.\n",
        "\n",
        "Use the `support_` attribute of the `RFE` class to get an array containing boolean values wherein `True` denotes the feature selected by RFE. You can also use the `ranking_` attribute of the `RFE` class to to get the rankings of the features. The features selected by `RFE` are ranked $1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyKg96LKAvxr"
      },
      "source": [
        "# Use RFE to eliminate few features from the dataset.\n",
        "# Import RFE.\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# RFE with 10 features.\n",
        "skl_lin_reg = LinearRegression()\n",
        "rfe1 = RFE(skl_lin_reg, n_features_to_select = 10)\n",
        "\n",
        "# Fit with 10 features.\n",
        "rfe1.fit(X_train[major_features.keys()], y_train)\n",
        "\n",
        "# Print the attributes.\n",
        "print(major_features.keys(), \"\\n\") # List of features out of which 10 best featuers are to be selected by RFE.\n",
        "print(rfe1.support_, \"\\n\") # Array containing the boolean values\n",
        "print(rfe1.ranking_, \"\\n\") # Ranking of the features selected by RFE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvYP0SwKBAQE"
      },
      "source": [
        "# Build a linear regression model using the 'statsmodels.api' module having the above 10 features selected using RFE.\n",
        "# Import the 'statsmodels.api' module.\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Subset the train set such that it contains only the above 10 selected features.\n",
        "rfe_features = X_train[major_features.keys()].columns[rfe1.support_]\n",
        "X_train_rfe1 = X_train[rfe_features]\n",
        "\n",
        "# Add the 'const' column to the features set.\n",
        "X_train_rfe1 = sm.add_constant(X_train_rfe1)\n",
        "\n",
        "# Fit the model with 10 features selected by RFE.\n",
        "sm_lin_reg1 = sm.OLS(y_train, X_train_rfe1).fit()\n",
        "print(sm_lin_reg1.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVpyMD_tLskx"
      },
      "source": [
        "The $R^2$ and adjusted $R^2$ values are close to $1$ which is a good sign. However, the p-values for a few features are greater than 0.05 which is not a good sign. So let's also calculate the VIF values before you further eliminate more features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWYhQwLNBoyP"
      },
      "source": [
        "# Check for the VIF values of the 10 features selected by RFE above.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
        "vif = pd.DataFrame()\n",
        "vif['Features'] = X_train_rfe1.columns\n",
        "vif['VIF'] = [variance_inflation_factor(X_train_rfe1.values, i) for i in range(X_train_rfe1.shape[1])]\n",
        "vif['VIF'] = round(vif['VIF'], 2)\n",
        "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
        "vif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AauTo3Qp9Tqs"
      },
      "source": [
        "Here, you can see that a couple features have high VIF values which means there's a high probability of multicollinearity even though the `statsmodels.api` didn't throw a multicollinearity warning message. You need to remove this multicollinearity either by one-by-one eliminating features or by selecting the fewer features. Let's go with the latter approach.\n",
        "\n",
        "\n",
        "**Note:** As a guideline, to further eliminate the features:\n",
        "\n",
        "1. Calculate the number of features having high p-values. Let's say this is 5.\n",
        "\n",
        "2. Calculate the number of features having high VIF values. Let's say this is 3.\n",
        "\n",
        "3. The number of features to be further eliminated would be the maximum of the numbers obtained in steps 1 and 2. In this case, $\\text{max}\\{5, 3\\} = 5$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyCa-EZ9919L"
      },
      "source": [
        "# Use RFE to eliminate few features from the dataset.\n",
        "# RFE with 5 features.\n",
        "skl_lin_reg2 = LinearRegression()\n",
        "rfe2 = RFE(skl_lin_reg2, n_features_to_select = 5)\n",
        "\n",
        "# Fit with 5 features.\n",
        "rfe2.fit(X_train[major_features.keys()], y_train)\n",
        "\n",
        "# Print the attributes.\n",
        "print(major_features.keys(), \"\\n\")\n",
        "print(rfe2.support_, \"\\n\")\n",
        "print(rfe2.ranking_, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_usSx1f6-GHm"
      },
      "source": [
        "# Build the linear regression model again using the most recently selected features by RFE\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Subset the train set such that it contains only the above 5 selected features.\n",
        "rfe_features = X_train[major_features.keys()].columns[rfe2.support_]\n",
        "X_train_rfe2 = X_train[rfe_features]\n",
        "\n",
        "# Add the 'const' column to the features set.\n",
        "X_train_rfe2 = sm.add_constant(X_train_rfe2)\n",
        "\n",
        "# Fit the model with 5 features.\n",
        "sm_lin_reg2 = sm.OLS(y_train, X_train_rfe2).fit()\n",
        "print(sm_lin_reg2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Hw7uzfPasE"
      },
      "source": [
        "As you can see, the $R^2$ and adjusted $R^2$ values are pretty high. The p-values for all the 5 features selected are less than 0.05. These are great signs. Now let's calculate the VIF values for these 5 features to see if there is negligible multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8jJLtoq-UBY"
      },
      "source": [
        "# Check for the VIF values of the feature variables.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
        "vif = pd.DataFrame()\n",
        "vif['Features'] = X_train_rfe2.columns\n",
        "vif['VIF'] = [variance_inflation_factor(X_train_rfe2.values, i) for i in range(X_train_rfe2.shape[1])]\n",
        "vif['VIF'] = round(vif['VIF'], 2)\n",
        "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
        "vif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKCxYZgg-xKz"
      },
      "source": [
        "As you can see, the VIF values for all the features is less than 10. Hence, we can safely assume that the multicollinearity between these features is negligible.\n",
        "\n",
        "Let's stop here. In the next class, we will evaluate this linear regression model further using the MAE, MSE, RMSE parameters and also analyse the residuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6a2op5DP9jv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ewSATK8tegw"
      },
      "source": [
        "#### Activity 1: Final Features\n",
        "\n",
        "Now that we have obtained the features that has satisfactory  VIF values and p-values, let's build linear regression model using the `sklearn` module to make predictions on the test set as well and evaluate the efficacy of the model using the MAE, MSE, RMSE parameters and also analyse the residuals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-228aiHeB0ba"
      },
      "source": [
        "# S1.1: Create a list of final features and build a linear regression model using the 'sklearn' module.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvEkQAHUuQWr"
      },
      "source": [
        "Now let's evaluate the efficacy of the model using the $R^2$, adjusted-$R^2$ MAE, MSE and RMSE parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FHxIh0-DzmF"
      },
      "source": [
        "# S1.2: Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rU5CxSluuKz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jujWiXrSuZV4"
      },
      "source": [
        "#### Activity 2: Mean-Squared Log Error (MSLE)\n",
        "\n",
        "When you are predicting large numbers (such as prices, length in millimeter scale etc), often the errors (difference between the actual and the predicted values) are also quite large. In such cases, the MSE, MSE and RMSE values are also large that might give us a false sense that the prediction model is poor. Hence, in such cases, it is best to use another parameter to analyse the errors. This parameter is called **Mean-Squared Log Error (MSLE)**.\n",
        "\n",
        "Mathematically, it is written as\n",
        "\n",
        "$$\\text{MSLE} = \\frac{1}{n}\\left( \\log{(y_\\text{actual} + 1)} - \\log{(y_\\text{predicted} + 1)} \\right)^2$$\n",
        "\n",
        "Before we further with MSLE, let's take quick at log or logarithm used in the formula.\n",
        "\n",
        "**What is natural logarithm or log?**\n",
        "\n",
        "Consider a number $32$. It can also be written as $2^5$ i.e.\n",
        "\n",
        "$$2^{5} = 32$$\n",
        "\n",
        "Here, the number $2$ is called **base**, the number $5$ is called **exponent (or index or power)**.\n",
        "\n",
        "The above mathematical statement can be written as **When $2$ raised to the power $5$, the number obtained is 32.**\n",
        "\n",
        "Now if you flip the above statement as **to what power the number 2 should be raised to so that the number obtained is 32**.\n",
        "\n",
        "This is flipped statement (or rather a question) is roughly the definition of a logarithm.\n",
        "\n",
        "Mathematically, it is written as\n",
        "\n",
        "$$\\log_2 32 = 5$$\n",
        "\n",
        "The implication\n",
        "\n",
        "$$2^{5} = 32 \\Leftrightarrow \\log_2 32 = 5$$\n",
        "\n",
        "is also true.\n",
        "\n",
        "In general, you can say **to what power a base (say $b$) should be raised to so that the number obtained is $c$.**\n",
        "\n",
        "The diagramatic of explanation of the above general definition is shown below.\n",
        "\n",
        "<img src = 'https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/log_explanation.png' width = 500>\n",
        "\n",
        "Let's look into the process stepwise:\n",
        "\n",
        "**Step 1:** For exponentiation multiply $b$ with itself $n$ times to get the result $c$\n",
        "\n",
        "$$b^n = b \\times b \\times b \\times b \\times \\dots \\times b \\space{} (n \\: \\text{times})$$\n",
        "\n",
        "For eg.,\n",
        "\n",
        "$$2^5 =  2 \\times 2 \\times 2 \\times 2 \\times 2$$\n",
        "\n",
        "**Step 2:** This can be written as when $b$ raise to the power of $n$ gives result $c$\n",
        "\n",
        "$$b^n = c$$\n",
        "\n",
        "For eg.,\n",
        "\n",
        "\n",
        "$$2^5 = 32$$\n",
        "\n",
        "\n",
        "**Note** Here $b$ and $n$ are known values and $c$ is unknown value. $b$ is the base value and $n$ is the exponent and $c$ is the product of exponentiation.\n",
        "\n",
        "**Step 3:** Now lets reverse the formula a bit, a known $c$ can be achieved by multiplying a known $b$ by itself unknown number of times. That means exponent, i.e. $n$ is unknown It can be written as\n",
        "\n",
        "$$\\color{blue}b^\\color{red}{n} = c$$\n",
        "\n",
        "**Step 4:** By solving the above formula, we get\n",
        "\n",
        "$$\\color{red}{n} = {\\log_\\color{blue}b}(c)$$\n",
        "\n",
        "For eg.,\n",
        "\n",
        "$$5 = \\log_2 32$$\n",
        "\n",
        "The above can be written as logarithm of $c$ to the base $b$ will tell us value of the exponent $n$ means **how many times should $b$ be multiplied with $n$ to achieve $c$**.\n",
        "\n",
        "In layman terms,\n",
        "\n",
        "1. **Exponentiation** is starting with a base number and multiplying it with itself a certain exponent times which will give us some target.\n",
        "\n",
        "  For example, if a task needs to be completed by putting in same amount of effort again and again, using exponentiation, we can find out if we apply the effort certain number of times what proportion of the task will be completed.\n",
        "\n",
        "2. **Logarithm** is starting with the base and target value and find out how many times the base must be multiplied by itself i.e. find the exponent value.\n",
        "\n",
        "Now you find out the value of $\\log_5 625$\n",
        "\n",
        " Let's get a look about the mathematical functions for exponentiation and logarithm in python.\n",
        "\n",
        " In python, `math` module has functions `pow()` and `log()` functions for exponentiation and logarithm. The syntax is as followed:\n",
        "\n",
        " >**Syntax:** `math.pow(x, y)`: `pow()` returns `x**y` (x to the power of y).\n",
        "\n",
        " >**Syntax:** `math.log(x[, base])`\n",
        "\n",
        "`log()` returns the natural logarithm to the base-e of the number x (if the base is not defined). If the base is defined, it returns the given base's logarithm of the number x.\n",
        "\n",
        "You will get to learn about logarithms in more detail in the subsequent classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUw4Am-8lmT-"
      },
      "source": [
        "# S2.1 Import math and use 'pow()' to calculate 32nd power of 2 and log of 4294967296 to the base 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFH8kM8VWxef"
      },
      "source": [
        "As it can be observed log and exponentiation are inverse of each other.\n",
        "\n",
        "Back to MSLE, it is the difference log of the  actual values and predictions.\n",
        "\n",
        "**Note:** The `1` added to both actual values and predictions in the MSLE formula is to avoid the taking the logarithm of `0` because **log of zero is not defined.**\n",
        "\n",
        "Let's calculate the MSLE of the regression model created above using the `mean_squared_log_errror()` from the `sklearn.metrics` module using the syntax below:\n",
        "\n",
        "> **Syntax:** `mean_squared_log_error(y_true, y_pred)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh4xAk3juecZ"
      },
      "source": [
        "# S2.2: Evaluate the linear regression model based on MSLE.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyHYM3Woafne"
      },
      "source": [
        "MSLE returns the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "It means that MSLE will treat small true and predicted values difference which will also be small approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "The MSLE is low which is good for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gKXrWaKwoVT"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVG0iroNsGLf"
      },
      "source": [
        "#### Activity 3: Residual Analysis\n",
        "\n",
        "Let's now analyse the residuals (or errors) to further evaluate the efficay of the model.\n",
        "\n",
        "**Homoscedasticity**\n",
        "\n",
        "Let's check if the error terms are normally distributed. **For a line to be the best fit line, the mean of random errors (differene between the actual and predicted values) or mean of errors (or $\\epsilon$) should be 0.**\n",
        "\n",
        "Let us plot the histogram of the error terms and see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOezUKVtCiaa"
      },
      "source": [
        "# S3.1: Create a histogram for the errors obtained in the predicted values for the train set.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnF9aiMPyduh"
      },
      "source": [
        "The mean of residuals is zero for the train set which is exactly what we want for a reliable linear regression model.\n",
        "\n",
        "**Homoscedasticity**\n",
        "\n",
        "In addition to checking for the mean of errors, you also need to check for the Homoscedasticity.\n",
        "\n",
        "Again as you know for homoscedasticity, we need to check the trend in the scatter plot between the errors and predictions. **There should not be a trend** because if there is a trend then it means the variance changes or varies as increase or decrease the actual values.\n",
        "\n",
        "Let's create a scatter plot between the errors and predictions for the train set to see whether there is some kind of relationship between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOMJDBk1DfyO"
      },
      "source": [
        "# S3.2: Create a scatter plot between 'train_residuals' and 'y_train'.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utks0CPXymVo"
      },
      "source": [
        "The scatter plot between the residuals and the actual prices for the train set doesn't follow any kind of pattern. The dots are randomly scattered which is again exactly what we want.\n",
        "\n",
        "**Scatter Plot Between Actual & Predicted Values**\n",
        "\n",
        "Let's create a scatter plot between the actual values and predictions for the train set to see whether there is some kind of relationship between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51zDAR5sIBgr"
      },
      "source": [
        "# S3.3: Create a scatter plot for actual values and predictions of the training data set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb99Rh93y2OE"
      },
      "source": [
        "This scatter plot between the actual and predicted prices follow almost a straight line which suggests the predicted prices are close to the actual prices of a car.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrFgj8XXw7Sb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNrWpUO0w49P"
      },
      "source": [
        "#### Activity 4: Prediction on Test Set\n",
        "\n",
        "Let's repeat the model evaluation using the test set.\n",
        "\n",
        "Let's first create the `X_test` dataset using the same features finalized for the training data `X_train_final` using RFE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO95Q2IxFzkn"
      },
      "source": [
        "# S4.1: Predict the car prices on the test set.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-spmztLECA0"
      },
      "source": [
        "Let us get the predictions for the testing data using the recent model and `predict()` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rZdybWlFQPC"
      },
      "source": [
        "# S4.2: Create a variable and store the predictions of the testing dataset returned by `predict()` in it.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JK_Seb0FSDc"
      },
      "source": [
        "Now let's evaluate the efficacy of the model using the $R^2$, adjusted-$R^2$ MAE, MSE, RMSE and MSLE parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzE_REsuMGQZ"
      },
      "source": [
        "# S4.3: Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error', 'mean_squared_log_error' functions of the 'sklearn' module.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zqGMXyAFmkt"
      },
      "source": [
        "Let us plot the histogram of the error terms and predictions of the testing data and see what it looks like to evaluate the model on the residual analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkzY6ZcvMhR0"
      },
      "source": [
        "# S4.3: Create a histogram for the errors obtained in the predicted values for the testing set.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyh79zzYzMcH"
      },
      "source": [
        "The mean of residuals should have been zero. But the MSLE is low. So, we can still trust the reliability of the linear regression model that we built.\n",
        "\n",
        "\n",
        "Let us observe the trend between the actual prices and the residuals for the test set using the scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbZLSbPYM52g"
      },
      "source": [
        "# S4.4: Create a scatter plot for the errors obtained in the actual values for the test set for checking homoscedasticity .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8dsBhCNzc7R"
      },
      "source": [
        "The scatter plot between the residuals and the actual prices for the test set as well doesn't follow any kind of pattern. The dots are randomly scattered which is again exactly what we want.\n",
        "\n",
        "\n",
        "Let us observe the trend between the actual values and the predictions of the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WiBsw-gM5pk"
      },
      "source": [
        "# S4.5: Create a scatter plot for the actual values and the predictions of the testing dataset.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhV9p3rRzjSB"
      },
      "source": [
        "This scatter plot between the actual and predicted prices follow somewhat straight line which suggests that most of the predicted prices are close to the actual prices of a car.  \n",
        "\n",
        "So finally the most important features and their corresponding regression coeffients are\n",
        "\n",
        "|Feature|Coefficient|\n",
        "|-|-|\n",
        "|`carwidth`|1696.2271|\n",
        "|`enginesize`|2636.6328|\n",
        "|`horsepower`|2363.8354|\n",
        "|`drivewheel_fwd`|-2016.6827|\n",
        "|`car_company_buick`|7984.8761|\n",
        "\n",
        "Hence, the regression equation is\n",
        "\n",
        "$$Y = 14360 + 1696.2271x_1 + 2636.6328x_2 + 2363.8354x_3 - 2016.6827x_4 + 7984.8761x_5$$\n",
        "\n",
        "where\n",
        "\n",
        "- $Y$ is the price of a car\n",
        "\n",
        "- $x_1$ is `carwidth`\n",
        "\n",
        "- $x_2$ is `enginesize`\n",
        "\n",
        "- $x_3$ is `horsepower`\n",
        "\n",
        "- $x_4$ is `drivewheel_fwd`\n",
        "\n",
        "- $x_5$ is `car_company_buick`\n",
        "\n",
        "This concludes linear regression. Let's stop here. from the next class onwards, you will learn another classification-based machine learning algorithm called logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmAe4h8a6d5u"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G12lfVy1htRZ"
      },
      "source": [
        "### **Project**\n",
        "\n",
        "You can now attempt the **Capstone Project - House Price Prediction** on your own.\n",
        "\n",
        "\n",
        "**Capstone Project - House Price Prediction**: https://colab.research.google.com/drive/1rNOLaLj4FrAYjjuaG2cpPhky5ERhdxry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTl7wBX5hsY_"
      },
      "source": [
        "---"
      ]
    }
  ]
}