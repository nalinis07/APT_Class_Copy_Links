{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalinis07/APT_Class_Copy_Links/blob/MASTER/AT_Lesson_74_Class_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nreGKcypHo1M"
      },
      "source": [
        "# Lesson 74: Logistic Regression - Decision Boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze9uSJt-I-0E"
      },
      "source": [
        "### Teacher-Student Activities\n",
        "\n",
        "In the previous class, you learnt to build a logistic regression model purely using the sigmoid function and predicted the outcomes i.e. the patients having and not having heart disease by choosing different threshold values.\n",
        "\n",
        "In this class, you will create a linear function using the features in the dataset and pass it as an input to the sigmoid function. However, we will take a small detour from the heart disease prediction dataset and choose the ideal data to understand the rationale behind building a linear function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSyqQS5T8Y1D"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmj2glW8aKa"
      },
      "source": [
        "#### Activity 1: Create Dummy Dataset^\n",
        "\n",
        "Let's create a dummy dataset having two columns representing two independent variables, another column representing the target and a total of 1000 records (or rows).\n",
        "\n",
        "You will shortly get to know the reason behind the creation of a dummy data-frame.\n",
        "\n",
        "To create a dummy data-frame, firstly, create two arrays using the `make_blob()` function of the `sklearn.datasets` module. The syntax for the `make_blob()` function is as follows:\n",
        "\n",
        "**Syntax:** `make_blobs(n_samples, centers, n_features, random_state, cluster_std)`\n",
        "\n",
        "where\n",
        "\n",
        "- `n_samples` determines the number of records to be generated in a dataset\n",
        "\n",
        "- `centers` determines the number of classes to be generated (or labels) for the target column\n",
        "\n",
        "- `n_features` determines the number of features (or independent variables) to be generated in the dataset\n",
        "\n",
        "- `random_state` determines the random number generation for dataset creation. An integer value of the `random_state` parameter will produce the same results across different function calls. Popular integer random seeds are 0 and 42.\n",
        "\n",
        "- `cluster_std` determines the standard deviation of the clusters.\n",
        "\n",
        "The function will return two arrays. You can store these two arrays in two variables, say `features_array` and `target_array`, where the former contains features (or independent variables) and the latter contains the target or (dependent variable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIBpSYJB8hsx"
      },
      "source": [
        "# T1.1: Create two arrays using the 'make_blobs()' function and store them in the 'features_array' and 'target_array' variables.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iJ3A_FGB5zk"
      },
      "source": [
        "**Note:** The `make_blobs()` function can take in more parameters that can be passed to create more customised data. You may refer to the following document:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
        "\n",
        "You can find out the object-type of the arrays created by the `make_blob()` function and the number of rows and columns in them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6M4iKHo8kQ1"
      },
      "source": [
        "# S1.1: Find out the object-type of the arrays created by the 'make_blob()' function and the number of rows and columns in them.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmNp-fZQC-qX"
      },
      "source": [
        "Now that you have created two arrays, as the next step to create a dummy data-frame, create a Python dictionary from the two arrays and then a Pandas DataFrame from the dictionary using the `from_dict()` function of the `pandas` module.\n",
        "\n",
        "**Syntax:** `pd.DataFrame.from_dict(some_dictionary)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M30S7jCeDGEn"
      },
      "source": [
        "# S1.2: Create a Pandas DataFrame containing the items from the 'features_array' and 'target_array' arrays.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-o7flxsERh-"
      },
      "source": [
        "Now we have a `pandas` DataFrame having 1000 records, 3 columns out of which 2 columns represent independent variables and 1 represent target.\n",
        "\n",
        "Let's count the number of 0s and 1s in the `target` column of the `dummy_df` DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPLsuU56EzRn"
      },
      "source": [
        "# S1.3: Display the number of occurrences of each label in the 'target' column.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub8VFodBX-3G"
      },
      "source": [
        "As you can see, both the labels (0s and 1s) are equal in number in the `target` column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16VQyfuGX-X4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U-OZvfIE7W_"
      },
      "source": [
        "#### Activity 2: Cluster Visualisation^^\n",
        "\n",
        "If you create a scatter plot between the columns `col 1` and `col 2` for the labels 0 and 1, then you will see two clusters of dots (or points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzn4GxEBFnMg"
      },
      "source": [
        "# S2.1: Create a scatter plot between 'col 1' and 'col 2' columns separately for both the classes in the same plot.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRnZaJhhIquq"
      },
      "source": [
        "This is the reason behind creating a dummy data-frame. Here, you can clearly see that two different coloured clusters are formed for the two different labels (or classes). You can draw an imaginary straight line to separate the two clusters from each other.\n",
        "\n",
        "For the sake of better understanding, let's assume that the equation of the straight line separating the two clusters is $x = 1$ (*even if the equation is incorrect*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6iHY9Iui8c_"
      },
      "source": [
        "# S2.2: Plot the line x = 1 in the above scatter plot.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCiXrYSix3-"
      },
      "source": [
        "So any point on the left-hand side of the line can be classified as 0 and any point on the right-hand side of the line can be classified as 1. This imaginary straight line separating the two clusters and classifying the points as 0 and 1 is called the **decision boundary**.\n",
        "\n",
        "Keeping the same idea in mind, you can conclude that you need more and more features in a dataset so that different clusters could be formed for different classes and these clusters can be separated by a decision boundary.\n",
        "\n",
        "To keep things simple, for this dummy dataset, we will consider only two features (or independent variables) to have a two-dimensional decision boundary. If we consider more than two features, then we will have to deal with a higher-dimensional decision boundary which is difficult to visualise on a two-dimensional plane.\n",
        "\n",
        "Your next task is to find out the equation of this decision boundary. The required equation will be the input to the sigmoid function to get the probabilities. Then w.r.t. a threshold value, you can classify the outcomes as 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TQbOSKLukhl"
      },
      "source": [
        "**Equation of a Straight Line in General Form**\n",
        "\n",
        "Before continuing ahead with the decision boundary, let's learn the equation of a straight line in the general form.\n",
        "\n",
        "You already know that the equation of a straight line is given as\n",
        "\n",
        "$$y = mx + c$$\n",
        "\n",
        "where\n",
        "\n",
        "- $m$ is the slope of a line and\n",
        "\n",
        "- $c$ is the intercept made by the line on the $y$-axis\n",
        "\n",
        "The equation of a straight line in the **general form** is given as\n",
        "\n",
        "$$ax + by + c = 0$$\n",
        "\n",
        "where\n",
        "\n",
        "- $a$ is the coefficient of $x$,\n",
        "\n",
        "- $b$ is the coefficient of $y$, and\n",
        "\n",
        "- $c$ is some arbitrary constant\n",
        "\n",
        "If you rewrite the above general equation as follows\n",
        "\n",
        "$$by = -ax - c$$\n",
        "$$\\Rightarrow y = - \\left(\\frac{a}{b}\\right) x - \\frac{c}{b}$$\n",
        "\n",
        "then you get the equation of the straight line back in the **slope-intercept** form where\n",
        "\n",
        "- the slope is $-\\frac{a}{b}$; more precisely $-\\frac{\\text{coefficient of } x}{\\text{coefficient of }y}$ and\n",
        "\n",
        "- the intercept-made by the line on the $y$-axis is $-\\frac{c}{b}$; more precisely $-\\frac{\\text{arbitrary constant}}{\\text{coefficient of }y}$\n",
        "\n",
        "**Note:** The $c$ in $y = mx + c$ is **not** the same as the $c$ in $ax + by + c = 0$\n",
        "\n",
        "With the knowledge of the general form of the equation of a straight line, you can assume that the equation of the decision boundary is\n",
        "\n",
        "$$\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 = 0$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\\beta_1 x_1 + \\beta_2 x_2 + \\beta_0 = 0$$\n",
        "\n",
        "If you compare it with the equation of a straight line in the general form,\n",
        "\n",
        "$$ax + by + c = 0$$\n",
        "\n",
        "then\n",
        "\n",
        "$$\\beta_1 = a, x_1 = x, \\beta_2 = b, x_2 = y \\space{} \\text{and} \\space{} \\beta_0 = c$$\n",
        "\n",
        "In this case, $x_1$ and $x_2$ represent the `col 1` and `col 2` columns respectively.\n",
        "\n",
        "To find this decision boundary (the straight line given by $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 = 0$), you need to build a logistic regression model between $x_1$ and $x_2$. From this model, you will get the coefficients i.e., $\\beta_0, \\beta_1$ and $\\beta_2$.\n",
        "\n",
        "So now, calculate the coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etrA_2UW4DCj"
      },
      "source": [
        "# S2.3: Calculate the coefficients by building a logistic regression model using the 'sklearn' model.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NquAyyRp8qn"
      },
      "source": [
        "Now that you have the required coefficients, you can get the equation of the decision boundary (in this case; a straight line).\n",
        "\n",
        "Next, let's plot the decision boundary (or straight line) in the above scatter plot. To plot the line, you can\n",
        "\n",
        "- plot the `col 1` values on the $x$-axis\n",
        "\n",
        "- calculate the `col 2` values from the $\\beta_0, \\beta_1, \\beta_2$ and the `col 1` values using the expression\n",
        "  $$x_2 = - \\left( \\frac{\\beta_1}{\\beta_2} \\right) x_1 - \\frac{\\beta_0}{\\beta_2}$$\n",
        "  or\n",
        "  $$x_2 = - \\left( \\frac{\\beta_1 x_1 + \\beta_0}{\\beta_2} \\right)$$\n",
        "  or\n",
        "  $$x_2 =  \\left( \\frac{- \\beta_0-\\beta_1 x_1}{\\beta_2} \\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xD2tqBK8K_i"
      },
      "source": [
        "# S2.4: Plot the calculated regression line in above scatter plot.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4H4f1aMfNz1"
      },
      "source": [
        "As you can see, the decision boundary clearly separates the two clusters for the two classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX8Ug8Wuryxu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyj3OiTtQG5u"
      },
      "source": [
        "#### Activity 3: Predictions^^^\n",
        "\n",
        "Now, your next task is to predict the `target` values based on the `col 1` and `col 2` values using the sigmoid function and some threshold value, say 0.5.\n",
        "\n",
        "Let's rewrite the sigmoid function as\n",
        "$$p =  \\frac{1}{1 + e^{-h}}$$\n",
        "\n",
        "where\n",
        "- $p$ is the probability value between 0 and 1\n",
        "- $h$ is some relationship between $x_1, x_2, x_3, \\dots, x_n$\n",
        "- $x_1, x_2, x_3, \\dots, x_n$ are $n \\in N$ feature(s) in a dataset\n",
        "\n",
        "In this case of dummy dataset, $h = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$\n",
        "\n",
        "You can also read $h$ as a **hypothesis**. In a broader sense, to predict the labels 0 and 1 through logistic regression for the dummy dataset, you need to pass a hypothesis as an input to the sigmoid function. In this case, the hypothesis is that the `col 1` and `col 2` values have a linear relationship ($h = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$) that predicts the labels 0 and 1.\n",
        "\n",
        "$$\\therefore p =\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)}}$$\n",
        "\n",
        "where $\\beta_0, \\beta_1, \\beta_2$ are coefficients and $x_1, x_2$ are the independent variables.\n",
        "\n",
        "The above expression can also be written as\n",
        "\n",
        "$$p =\\frac{1}{1 + e^{-BX^T}}$$\n",
        "\n",
        "where\n",
        "\n",
        "- $X$ denotes the feature matrix i.e.\n",
        "\n",
        "  $$X = \\begin{bmatrix} 1 & x_{1,1} & x_{2,1} \\\\ 1 & x_{1,2} & x_{2,2}\\\\ 1 & x_{1,3} & x_{2,3} \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_{1,1000} & x_{2,1000} \\end{bmatrix}$$\n",
        "\n",
        "  Each item in the above matrix can be represented by $x_{i,j}$ where $x_{i,j}$ denotes the $j^{\\text{th}}$ item in the $i^{\\text{th}}$ column. Also the values of $i$ goes from $1$ to $2$ and the values of $j$ goes from $1$ to $1000$.\n",
        "\n",
        "- $B$ denotes the regression coefficients matrix i.e.\n",
        "\n",
        "  $$B = \\begin{bmatrix} \\beta_0 & \\beta_1 & \\beta_2\\end{bmatrix}$$\n",
        "\n",
        "Essentially, $$h = BX^T$$\n",
        "\n",
        "\n",
        "The above product after taking the transpose of the features matrix becomes\n",
        "\n",
        "$$h = \\begin{bmatrix} \\beta_0 & \\beta_1 & \\beta_2\\end{bmatrix} \\begin{bmatrix} 1 & 1 & 1 & \\dots & 1 \\\\ x_{1,1} & x_{1,2} & x_{1,3} & \\dots & x_{1,1000} \\\\ x_{2,1} & x_{2,2} & x_{2,3} & \\dots & x_{2,1000} \\end{bmatrix}$$\n",
        "\n",
        "So let's create the `sigmoid()` function in Python that takes the arrays containing the features and the regression coefficients as inputs and returns the probability values as output. But first, you need to add a column containing 1s to the features array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A5Oi179RG58"
      },
      "source": [
        "# S3.1: Add a column containing 1s to the features array.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aldZlz0YI90T"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "1. `np.ones(shape = (features_array.shape[0], 1)` generates a NumPy array containing 1s and has 1000 rows and 1 column.\n",
        "\n",
        "2. `np.append()` function adds the `features_array` to the array created in the first step column-wise (denoted by `axis = 1`). The general syntax is\n",
        "   `np.append(parent_matrix, matrix_to_be_added_to_the_parent_matrix)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT4Z_4sGQPxF"
      },
      "source": [
        "# S3.2 Create the 'sigmoid()' function.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfgd4g-7KSLv"
      },
      "source": [
        "**Note:** The matrix multiplication rules are applicable to the NumPy arrays as well.\n",
        "\n",
        "Now, let's create a scatter plot between the $h = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$ values and their corresponding sigmoid outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qBGQtKEULhd"
      },
      "source": [
        "# S3.3: Create a scatter plot between the 'h' values and their corresponding sigmoid outputs.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZkBAV5pNqif"
      },
      "source": [
        "As you can see, the above plot follows the sigmoid curve.\n",
        "\n",
        "Now, let's predict the target values w.r.t. the threshold value of 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJJiMlKv4TVJ"
      },
      "source": [
        "# S3.4: Predict the target values w.r.t. the threshold value of 0.5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXDnm2HvN6se"
      },
      "source": [
        "Finally, let's calculate the accuracy of the logistic regression model built through a confusion matrix, precision value, recall value and f1-score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZL2nJvX4n4f"
      },
      "source": [
        "# S3.5: Evaluate the accuracy of the logistic regression model built through a confusion matrix, precision value, recall value and f1-score.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O5XDqkTOV22"
      },
      "source": [
        "As you can see,\n",
        "- The FP and FN values in the confusion matrix are 0\n",
        "- The precision and recall values are 1\n",
        "- The f1-score is also 1\n",
        "\n",
        "This clearly shows that the decision boundary accurately separates the labels (or classes) with 100% accuracy.\n",
        "\n",
        "Let's stop here. In the next class, we will study more concepts on logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx3sCQvxEZPH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZfQUp_SjJhK"
      },
      "source": [
        "### **Project**\n",
        "\n",
        "You can now attempt the **Applied Tech Project 74 - Logistic Regression - Decision Boundary** on your own.\n",
        "\n",
        "\n",
        "**Applied Tech Project 74 - Logistic Regression - Decision Boundary**: https://colab.research.google.com/drive/1oe3AGq5RqGL38DLSPSZ73Gk5VlwNpJIn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BwVOdF_jIsF"
      },
      "source": [
        "---"
      ]
    }
  ]
}