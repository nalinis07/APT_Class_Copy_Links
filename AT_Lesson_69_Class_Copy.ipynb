{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalinis07/APT_Class_Copy_Links/blob/MASTER/AT_Lesson_69_Class_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN_oEsXfM_LV"
      },
      "source": [
        "# Lesson 69: Car Price Prediction - Interpreting p-value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFjOAv8nAhzQ"
      },
      "source": [
        "### Teacher-Student Activities\n",
        "\n",
        "In the previous class, you learned feature encoding using the one-hot encoding and dummy coding processes. You also learned to calculate the adjusted R-squared value to evaluate a linear regression model.\n",
        "\n",
        "In this class, you will learn the concept of p-value which will help you to determine which features are significant to the dataset and which are not so that you can create your model with those features which are significantly contributing in prediction.\n",
        "\n",
        "Let's quickly run the codes covered in the previous classes and begin this session from **Activity 1: Understanding Hypothesis Testing** section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rqO31eQB5O-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-uhY3ryaj4N"
      },
      "source": [
        "### Problem Statement\n",
        "\n",
        "Build a linear regression model to predict prices of cars based on its technical specifications such as car manufacturer, its engine capacity, fuel efficiency, body-type etc.\n",
        "\n",
        "**Dataset Description:**\n",
        "\n",
        "The dataset contains 205 rows and 26 columns. Each column represents an attribute of a car as described in the table below.\n",
        "\n",
        "|Sr No.|Attribute|Attribute Information|\n",
        "|-|-|-|\n",
        "|1|Car_ID|Unique id of each car (Interger)|\n",
        "|2|Symboling|Assigned insurance risk rating; a value of +3 indicates that the car is risky; -3 suggests that it is probably a safe car (Categorical)|\n",
        "|3|carCompany|Name of car company (Categorical)|\n",
        "|4|fueltype| fuel-type i.e. petrol or diesel (Categorical)|\n",
        "|5|aspiration|Aspiration used in a car (Categorical)|\n",
        "|6|doornumber|Number of doors in a car (Categorical)|\n",
        "|7|carbody|Body-type of a car (Categorical)|\n",
        "|8|drivewheel|Type of drive wheel (Categorical)|\n",
        "|9|enginelocation|Location of car engine (Categorical)|\n",
        "|10|wheelbase|Weelbase of car (Numeric)|\n",
        "|11|carlength|Length of car (Numeric)|\n",
        "|12|carwidth|Width of car (Numeric)|\n",
        "|13|carheight|Height of car (Numeric)|\n",
        "|14|curbweight|The weight of a car without occupants or baggage (Numeric)|\n",
        "|15|enginetype|Type of engine (Categorical)|\n",
        "|16|cylindernumber|Number of cylinders placed in the car engine (Categorical)||17|enginesize|Capacity of an engine (Numeric)|\n",
        "|18|fuelsystem|Fuel system of a car (Categorical)|\n",
        "|19|boreratio|Bore ratio of car (Numeric)|\n",
        "|20|stroke|Stroke or volume inside the engine (Numeric)|\n",
        "|21|compressionratio|Compression ratio of an engine (Numeric)|\n",
        "|22|horsepower|Power output of an engine (Numeric)|\n",
        "|23|peakrpm|Peak revolutions per minute (Numeric)|\n",
        "|24|citympg|Mileage in city (Numeric)|\n",
        "|25|highwaympg|Mileage on highway (Numeric)|\n",
        "|26|price(Dependent variable)|Price of a car (Numeric)|\n",
        "\n",
        "This data set consists of three types of entities:\n",
        "\n",
        "- the specification of an auto in terms of various characteristics,\n",
        "\n",
        "- its assigned insurance risk rating,\n",
        "\n",
        "- its normalised losses in use as compared to other cars.\n",
        "\n",
        "The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process **symboling**. A value of $+3$ indicates that the auto is risky, $-3$ that it is probably pretty safe.\n",
        "\n",
        "The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality etc.), and represents the average loss per car per year.\n",
        "\n",
        "**Note:** Several of the attributes in the database could be used as a \"class\" attribute.\n",
        "\n",
        "**Dataset source:** https://archive.ics.uci.edu/ml/datasets/Automobile\n",
        "\n",
        "\n",
        "The above dataset consists of data taken from 1985 Ward's Automotive Yearbook. Here's the list of original sources of the data:\n",
        "\n",
        "1. 1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook.\n",
        "\n",
        "2. Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038\n",
        "\n",
        "3. Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZEA9P6hDG28"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVeYBHNDHh8"
      },
      "source": [
        "#### Recap\n",
        "\n",
        "https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/car-prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f2emlnJM56A"
      },
      "source": [
        "# Import the modules, read the dataset and create a Pandas DataFrame.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Read the dataset\n",
        "cars_df = pd.read_csv(\"https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/car-prices.csv\")\n",
        "\n",
        "# Data Cleaning\n",
        "# Extract the name of the manufactures from the car names and display the first 25 cars to verify whether names are extracted successfully.\n",
        "car_companies = pd.Series([car.split(\" \")[0] for car in cars_df['CarName']], index = cars_df.index)\n",
        "\n",
        "# Create a new column named 'car_company'. It should store the company names of a the cars.\n",
        "cars_df['car_company'] = car_companies\n",
        "\n",
        "# Replace the misspelled 'car_company' names with their correct names.\n",
        "# volkswagen\n",
        "cars_df.loc[(cars_df['car_company'] == \"vw\") | (cars_df['car_company'] == \"vokswagen\"), 'car_company'] = 'volkswagen'\n",
        "\n",
        "# porsche\n",
        "cars_df.loc[cars_df['car_company'] == \"porcshce\", 'car_company'] = 'porsche'\n",
        "\n",
        "# toyota\n",
        "cars_df.loc[cars_df['car_company'] == \"toyouta\", 'car_company'] = 'toyota'\n",
        "\n",
        "# nissan\n",
        "cars_df.loc[cars_df['car_company'] == \"Nissan\", 'car_company'] = 'nissan'\n",
        "\n",
        "# mazda\n",
        "cars_df.loc[cars_df['car_company'] == \"maxda\", 'car_company'] = 'mazda'\n",
        "\n",
        "# Drop 'CarName' column from the 'cars_df' DataFrame.\n",
        "cars_df.drop(columns= ['CarName'], axis = 1, inplace = True)\n",
        "\n",
        "# Data Preparation\n",
        "# Extract all the numeric (float and int type) columns from the dataset.\n",
        "cars_numeric_df = cars_df.select_dtypes(include = ['int64', 'float64'])\n",
        "\n",
        "# Drop the 'car_ID' column from the 'cars_numeric_df' DataFrame.\n",
        "cars_numeric_df.drop(columns = ['car_ID'], axis = 1, inplace = True)\n",
        "\n",
        "# Mapping Categorical Values\n",
        "# Map the values of the 'doornumber' and 'cylindernumber' columns to their corresponding numeric values.\n",
        "words_dict = {\"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\": 6, \"eight\": 8, \"twelve\": 12}\n",
        "def num_map(series):\n",
        "    return series.map(words_dict)\n",
        "\n",
        "# Applying the function to the two columns\n",
        "cars_df[['cylindernumber', 'doornumber']] = cars_df[['cylindernumber', 'doornumber']].apply(num_map, axis = 1)\n",
        "\n",
        "# Feature Encoding\n",
        "# Create dummy variables for the 'carbody' columns.\n",
        "car_body_dummies = pd.get_dummies(cars_df['carbody'], dtype = int)\n",
        "\n",
        "# Create dummy variables for the 'carbody' columns with 1 column less.\n",
        "car_body_new_dummies = pd.get_dummies(cars_df['carbody'], drop_first = True, dtype = int)\n",
        "\n",
        "# Create a DataFrame containing all the non-numeric type features.\n",
        "cars_categorical_df = cars_df.select_dtypes(include = ['object'])\n",
        "\n",
        "# Get dummy variables for all the categorical type columns using the dummy coding process.\n",
        "cars_dummies_df = pd.get_dummies(cars_categorical_df, drop_first = True, dtype = int)\n",
        "\n",
        "# Drop the categorical type columns from the 'cars_df' DataFrame.\n",
        "cars_df.drop(list(cars_categorical_df.columns), axis = 1, inplace = True)\n",
        "\n",
        "# Concatenate the 'cars_df' and 'cars_dummies_df' DataFrames.\n",
        "cars_df = pd.concat([cars_df, cars_dummies_df], axis = 1)\n",
        "\n",
        "# Drop the 'car_ID' column\n",
        "cars_df.drop('car_ID', axis = 1, inplace = True)\n",
        "\n",
        "# Test-Train Split\n",
        "# Split the 'cars_df' Dataframe into the train and test sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(cars_df, test_size = 0.3, random_state = 42)\n",
        "\n",
        "# Create separate data-frames for the feature and target variables for both the train and test sets.\n",
        "features = list(cars_df.columns)\n",
        "features.remove('price')\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df['price']\n",
        "X_test = test_df[features]\n",
        "y_test = test_df['price']\n",
        "\n",
        "# Feature Scaling\n",
        "# Normalise only the numeric columns that were you had prior to any data-cleaning exercise.\n",
        "def standard_norm(series):\n",
        "  new_series = (series - series.mean()) / series.std()\n",
        "  return new_series\n",
        "\n",
        "# Normalising the features in the train and test sets.\n",
        "X_train[X_train.columns[:16]] = X_train[X_train.columns[:16]].apply(standard_norm, axis = 0)\n",
        "X_test[X_train.columns[:16]] = X_test[X_train.columns[:16]].apply(standard_norm, axis = 0)\n",
        "\n",
        "# Model Building\n",
        "# Build a linear regression model using all the features to predict car prices.\n",
        "import statsmodels.api as sm\n",
        "\n",
        "X_train_sm = sm.add_constant(X_train)\n",
        "lin_reg = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "# Print the summary of the linear regression report.\n",
        "print(lin_reg.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mebERAD9DtH7"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_cZ9JNVTL10"
      },
      "source": [
        "#### Activity 1: Understanding Hypothesis Testing\n",
        "\n",
        "From the summary report of the linear regression, you may observe that each feature variable has a **p-value** `(P>|t|)` associated with it. The p-value is one of the important statistics which can be used to eliminate features which are not relatively significant in our model. Before understanding the p-value concept, let us first explore the concept of hypothesis testing.\n",
        "\n",
        "**Hypothesis Testing**\n",
        "\n",
        "Hypothesis Testing is basically testing an assumption that we make about a parameter. This assumption may or may not be true. Eg., \"students having an affluent background are more likely to do well in academics in higher education\" is one such hypothesis.\n",
        "\n",
        "The steps followed in hypothesis testing are:\n",
        "\n",
        "1. An initial assumption or hypothesis is made.\n",
        "2. The validity of that hypothesis is tested.\n",
        "3. If the hypothesis is found to be true, it is accepted otherwise it is rejected.\n",
        "\n",
        "There are two types of hypothesis:\n",
        "\n",
        "1. **Null hypothesis:** denoted by $H_0$, is a general statement or an initial assumption which we make about a parameter.\n",
        "2. **Alternative hypothesis:** denoted by $H_1$ or $H_a$, It is contrary to the null hypothesis. It is the hypothesis we would accept if our null hypothesis is found to be false.\n",
        "\n",
        "In hypothesis testing, we need to gather enough evidence to either accept or reject our null hypothesis. There are two types of hypothesis tests that can be used for multiple linear regression:\n",
        "- **F-test:** This test measures the overall significance of all the coefficients.\n",
        "- **T-test:** This test measures the significance of each individual coefficient.\n",
        "\n",
        "Let us first determine the overall significance of our model using the F-test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVqKWc0FWd_K"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtfCyEXmWqZd"
      },
      "source": [
        "#### Activity 2: F-Test\n",
        "\n",
        "The F-test is used to assess all the coefficients collectively. It validates whether any of the independent variables are significant. Let us apply F-test to the car price prediction model.\n",
        "\n",
        "The regression equation for the car price prediction model can be given as\n",
        "\n",
        "$$Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\dots + \\beta_{59} x_{59} + \\epsilon$$\n",
        "\n",
        "where,\n",
        "\n",
        " - $x_1$ is `symboling`\n",
        " - $x_2$ is `doornumber`\n",
        " - $x_3$ is `wheelbase`\n",
        "\n",
        " $\\vdots$   \n",
        "\n",
        " - $x_{59}$ is `wheelbase` and\n",
        " - $Y$ is the `price`\n",
        "\n",
        "\n",
        "**Step 1: Define null and alternative hypothesis**\n",
        "\n",
        "$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_{59} = 0$ i.e. all the regression coefficients are equal to zero.\n",
        "\n",
        "$H_1: \\beta_i \\neq 0$, i.e. at least one of the coefficient is not zero.\n",
        "\n",
        "- $H_0$ means that none of the feature or independent variables have a significant relationship with our target variable `price` and our model has no predictive capability.\n",
        "\n",
        "- $H_1$ means that at least one feature variable has a significant relationship with our target variable `price`.\n",
        "\n",
        "**Step 2: Calculate the test statistic value** (in case of F-test it is F-statistic value)\n",
        "\n",
        "It is calculated as\n",
        "\n",
        "$$F* = \\frac{\\textrm{explained variance}}{\\textrm{unexplained variance}} = \\frac{\\text{MSM}}{\\text{MSE}}$$\n",
        "\n",
        "where,\n",
        "\n",
        "- MSM is the Mean of Squares for Model\n",
        "- MSE is Mean of Squared Errors (or Residuals)\n",
        "\n",
        "Further, MSM  is calculated as\n",
        "\n",
        "$$\\text{MSM} = \\frac{\\text{SSM}}{\\text{DFM}}=\\frac{\\sum(y_{\\text{pred}} - \\bar{y})^2}{ p - 1}$$\n",
        "\n",
        "where,\n",
        "- SSM is the Sum of Squares for Model\n",
        "- DFM is Degrees of Freedom for Model\n",
        "- $p$ is the number of independent variables\n",
        "\n",
        "Similarly, MSE is calculated as:\n",
        "\n",
        "$$\\text{MSE} = \\frac{\\text{SSE}}{\\text{DFE}}=\\frac{\\sum(y - y_{\\text{pred}})^2}{ N - p}$$\n",
        "\n",
        "where,\n",
        "- SSE is the Sum of Squares for Errors\n",
        "- DFE is Degrees of Freedom for Errors\n",
        "- $N$ is number of instances (or rows) in the dataset\n",
        "\n",
        "Let's create `mean_sq_model()` and `mean_sq_error()` functions to calculate the MSM and MSE values using the above formulae respectively.\n",
        "\n",
        "**Note:** You can also obtain the MSM and MSE values using the `mse_model` and `mse_resid` attributes respectively of `statsmodels.api` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k87IbgJ2o-Br"
      },
      "source": [
        "# S2.1: Calculate N and p values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB_mnlQfq4HM"
      },
      "source": [
        "# S2.2: Create functions to calculate MSM and MSE values respectively.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljuui_SrREE"
      },
      "source": [
        "# S2.3: Calculate the MSM and MSE on the train sets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeXUsj5Ds9LN"
      },
      "source": [
        "Now let us calculate the F-statistic value using the\n",
        "\n",
        "$$F* = \\frac{\\text{MSM}}{\\text{MSE}}$$\n",
        "\n",
        " formula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6rtlBEtS_A"
      },
      "source": [
        "# S2.4: Calculate the F-statistic using the above formula.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF35-Ib0ux0R"
      },
      "source": [
        "**Step 3: Determine the p-value or probability value for the F-statistic**\n",
        "\n",
        "We can use manually calculate p-value for any test-statistic using the formula:\n",
        "$$\\textrm{p value} = 2 \\times  (1 - \\textrm{cdf}(|ts|))$$\n",
        "\n",
        "where $|ts|$ is the absolute value of test statistic (in this case, F-statistic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KUSyTPlvNrQ"
      },
      "source": [
        "# S2.5: Calculate p-value for F-statistic.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1tK5Pjpvab-"
      },
      "source": [
        "We can also directly calculate p-value for F-statistic using `f_pvalue` attribute of the `statsmodels.api` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqP5GjemvZ__"
      },
      "source": [
        "# S2.6: Calculate p-value using f_pvalue attribute\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPpkhyzvwazi"
      },
      "source": [
        "Thus, the F-statistic value is 61.81 and its p-value is 0.0. You may observe a slight difference in the `F-statistic` and `Prob (F-statistic)` values of the summary table as it works slightly different. This is to show that you can also derive F-statistic and its p-value directly from the summary table.\n",
        "\n",
        "**Step 4: Accept or reject null hypothesis based on the p-value**\n",
        "\n",
        "After determining the p-value, we either accept or reject our null hypothesis.\n",
        "\n",
        "If p-value is below 0.05, the null hypothesis will be rejected. Let's determine whether the p-value is below 0.05 or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kC7V6ex4V6U"
      },
      "source": [
        "# S2.7: Create a function to accept or reject null hypothesis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rex_Dpqw4iFx"
      },
      "source": [
        "The p-value that we obtained from F-test is equal to 0.00, so we can reject our null hypothesis and conclude that at least one of the independent variable has linear relationship with our target variable `price`. But, what is p-value?\n",
        "\n",
        "**What is meant by p-value?**\n",
        "\n",
        "The p-value is a probability value that helps us to determine whether our hypothesis is correct. The p-value for each feature tests the null hypothesis that there is no correlation between the feature and the target variable. Smaller the p-value, stronger is the evidence that you should reject null hypothesis. A p-value less than 0.05 is statistically significant. It indicates that there is less than 5% probability that the null hypothesis is correct. Therefore, we reject the null hypothesis, and accept the alternative hypothesis. However, a p-value greater than 0.05 indicates weak evidence and we fail to reject the null hypothesis.\n",
        "\n",
        "The F-test for our model rejected the null hypothesis and concluded that at least one feature variable is significant and our model definitely possess predictive capability. Now, we will perform **t-test** to determine which variables are significant in predicting the price of a car and which are not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRB8dcxb5lsA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCeaSQ-n7NXJ"
      },
      "source": [
        "#### Activity 3: T-Test\n",
        "\n",
        "After concluding from the F-test that at least one feature variable is significant, now we may want to know which variables are significant. For this, we can do a **t-test** to find out which independent variable is making a useful contribution in the prediction of the dependent variable.\n",
        "\n",
        "Remember, the regression equation for our model is:\n",
        "\n",
        "\n",
        "\n",
        "$$Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\dots + \\beta_{59} x_{59} + \\epsilon$$\n",
        "\n",
        "where,\n",
        " - $x_1$ is `symboling`\n",
        " - $x_2$ is `doornumber`\n",
        " - $x_3$ is `wheelbase`\n",
        "\n",
        " $\\vdots$   \n",
        "\n",
        " - $x_{59}$ is `wheelbase` and\n",
        " - $Y$ is the `price`\n",
        "\n",
        "For example, let us determine whether feature `symboling` is contributing significantly in the prediction of dependent variable `price`. We will follow the same steps as that of F-test.\n",
        "\n",
        "**Step 1:  Define the null and alternative hypothesis**\n",
        "\n",
        "$H_0:   \\beta_1 = 0$ i.e. `symboling` and `price` are not linearly related\n",
        "\n",
        "$H_1:   \\beta_1 \\neq 0$ i.e. `symboling` and `price` are linearly related\n",
        "\n",
        "**Step 2: Calculate the test statistic value** (in case of t-test, it is t-statistic value)\n",
        "\n",
        "The t-statistic is calculated as:\n",
        "\n",
        "$$t∗= \\frac{\\textrm{coefficient - hypothesized  value} }{\\textrm{standard  error  of  coefficient}}$$\n",
        "\n",
        "As the hypothesized value is usually 0,\n",
        "$$t∗= \\frac{\\textrm{coefficient} }{\\textrm{standard  error  of  coefficient}}$$\n",
        "\n",
        "For our example above, the t-statistic is:\n",
        "\n",
        "$$t∗= \\frac{\\beta_1 }{SE(\\beta_1)}$$\n",
        "\n",
        "The **standard error of coefficient (SE)** is an estimate of the standard deviation of the coefficient, the amount it varies across cases. Its formula is quite complicated.\n",
        "\n",
        "However, we can obtain standard error for every coefficient by using `bse` attribute of `statsmodels.api` module. The `b` in `bse` stands for the coefficient $\\beta$ and `se` for standard errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W74Uh6Oa82C7"
      },
      "source": [
        "# T3.1: Calculate the SE(beta_1) value.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fB6j9d094l8"
      },
      "source": [
        "# T3.2: Calculate t-statistic for beta_1 using the above formula.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXwNEAyT-X0K"
      },
      "source": [
        "**Step 3:  Determine the p-value or probability value for the t-statistic**\n",
        "\n",
        "After obtaining the t-statistic for $\\beta_1$, let's validate the null hypothesis by calculating the p-value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtTAnmdm-fJP"
      },
      "source": [
        "# T3.3: Calculate p-value based on t-statistic.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-bMn1aa-1cG"
      },
      "source": [
        "Thus the t-statistic value for $\\beta_1$ is -0.766 and its p-value is 0.443. You can also derive these values directly from the summary table.\n",
        "\n",
        "**STEP 4: Accept or reject null hypothesis based on the p-value**\n",
        "\n",
        "After determining the p-value, we either accept or reject our null hypothesis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx8Bm83l_HpY"
      },
      "source": [
        "# S3.1: Accept or reject null hypothesis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoEIHQEc_d7M"
      },
      "source": [
        "Since the p-value is above 0.05, the null hypothesis will be accepted. This means that `symboling` and `price` are not linearly related and `symboling` is not making a useful contribution in predicting the target variable `price`. Hence, we can remove this feature from our model.\n",
        "\n",
        "Similarly, let's perform t-test for the second feature `doornumber` to determine whether it is significant or not. For this, our null hypothesis and alternate hypothesis would be:\n",
        "\n",
        "$H_0:   \\beta_2 = 0$ i.e. `doornumber` and `price` are not linearly related\n",
        "\n",
        "$H_1:   \\beta_2 \\neq 0$ i.e. `doornumber` and `price` are  linearly related\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Nf7rJTAlI2"
      },
      "source": [
        "# S3.2: Calculate the SE(beta_2) value.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJdf1NrCAtaB"
      },
      "source": [
        "# S3.3: Calculate t-statistic for beta_2 using formula\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl1Nx1f9A6L-"
      },
      "source": [
        "# S3.4: Calculate p-value based on t-statistic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14smmrFRCEzJ"
      },
      "source": [
        "# S3.5: Accept or reject null hypothesis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqNFRfH3CLPu"
      },
      "source": [
        "Since the p-value is above 0.05, the null hypothesis will be accepted. This means that the feature `doornumber` is not making a useful contribution in predicting the target variable `price`. Hence, we can remove this feature from our model.\n",
        "\n",
        "Similarly, you can perform t-test for each independent variable and determine which variable is actually contributing in predicting the price of a car.\n",
        "\n",
        "You can obtain p-values for all features all at once either from the summary of linear regression report or by using `pvalues` attribute of Linear regression object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuTVDydcDhB9"
      },
      "source": [
        "# S3.6: Obtain p-values for all features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrL_v5-lDvcz"
      },
      "source": [
        "Let us obtain those features whose p-value is less than 0.05 and perform linear regression using the reduced features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1viwlEMcQIIc"
      },
      "source": [
        "# S3.7: Create a dataframe with Features and their corresponding p-values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnllksChTeQS"
      },
      "source": [
        "# S3.8: Drop those features whose p-value is greater than 0.05\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grmZkV4dTozs"
      },
      "source": [
        "As you can see, we created a dataframe with only significant features. Now let us again perform linear regression using the reduced features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi-4VYmEUOV8"
      },
      "source": [
        "# S3.9: Build a linear regression model again with reduced features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_5R_5YUXffV"
      },
      "source": [
        "# S3.10: Print the summary table for the above linear regression model.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64zwtdo2PBb0"
      },
      "source": [
        "We build the linear regression model again after removing all the features having the higher p-value and we still have a few features which have high p-values. This is not the right approach to tackle the high-values issue. Ideally, in the first iteration, we should remove only one feature having higher p-value, then rebuild the model, then again check for the p-value and then remove the next feature having highest p-value and so on.\n",
        "\n",
        "These iterations can become very long. We can reduce the number of iterations by finding out the most relevant features in the first go. In the next class, we will how to select such relevant features to reduce the number of iterations for building the most accurate linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijKY83ukqRon"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n41zL-OCskHC"
      },
      "source": [
        "### **Project**\n",
        "You can now attempt the **Applied Tech Project 69 - Car price prediction - Interpreting p-values** on your own.\n",
        "\n",
        "**Applied Tech Project 69 - Car price prediction - Interpreting p-values**: https://colab.research.google.com/drive/1fkUA8fMoDcZnia1kOeDgP0KnOoRUoU_6?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6YLADRwsyDk"
      },
      "source": [
        "---"
      ]
    }
  ]
}