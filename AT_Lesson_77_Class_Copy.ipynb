{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalinis07/APT_Class_Copy_Links/blob/MASTER/AT_Lesson_77_Class_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nreGKcypHo1M"
      },
      "source": [
        "# Lesson 77: Logistic Regression - Surface Plot & Partial Derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze9uSJt-I-0E"
      },
      "source": [
        "### Teacher-Student Activities\n",
        "\n",
        "In the previous class, you learnt to differentiate a function to find the slope of a tangent. You also learnt that the slope of a tangent is zero at the points of the maximum and minimum values of a curve as the tangent on those points are parallel to the $x$-axis. In this class, you will learn to create a surface plot which is a kind of three-dimensional plot to visualise the regularised cost function to visually locate the points of its maximum and minimum value.\n",
        "\n",
        "Let's quickly go through the activities covered in the previous class and begin this class from the **Activity 1: Surface Plot** section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSyqQS5T8Y1D"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csjlNKzAsQoF"
      },
      "source": [
        "#### Dummy Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4sy1DEmqYLE"
      },
      "source": [
        "# Dummy dataset creation using the 'make_blob()' function.\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "features_array, target_array = make_blobs(n_samples = 1000, centers = 2, n_features = 2, random_state = 42, cluster_std = 1.25)\n",
        "\n",
        "# Object-type of the arrays created by the 'make_blob()' function and the number of rows and columns in them.\n",
        "print(f\"The features array is an {type(features_array)} object.\\nThe target array is an {type(target_array)} object.\\n\")\n",
        "print(f\"The features array has {features_array.shape[0]} rows and {features_array.shape[1]} columns.\")\n",
        "print(f\"The target array has {target_array.shape[0]} rows and {len(target_array.shape)} column.\")\n",
        "\n",
        "# Pandas DataFrame creation.\n",
        "dummy_dict = {'col 1': [features_array[i][0] for i in range(features_array.shape[0])],\n",
        "             'col 2': [features_array[i][1] for i in range(features_array.shape[0])],\n",
        "             'target': target_array}\n",
        "\n",
        "dummy_df = pd.DataFrame.from_dict(dummy_dict)\n",
        "\n",
        "# The number of occurrences of each label in the 'target' column.\n",
        "print(f\"Target counts:\\n{dummy_df['target'].value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gqkkcLPqw8x"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QcbuNZlGK53"
      },
      "source": [
        "#### Activity 1: Surface Plot^\n",
        "\n",
        "Before differentiating the cost function\n",
        "\n",
        "$$J(\\beta) = -\\frac{1}{m} \\sum_{i = 1}^m \\{ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\} + \\frac{\\lambda}{2m} \\sum_{j = 1}^n \\beta_j^2$$\n",
        "\n",
        "to get the optimum values of $\\beta_0, \\beta_1$ and $\\beta_2$, let's visualise it for the different values of $\\beta_1$ and $\\beta_2$ only. In that case, the regularised cost function becomes\n",
        "\n",
        "$$J(\\beta_1, \\beta_2) = -\\frac{1}{m} \\sum_{i = 1}^m \\{ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\} + \\frac{\\lambda}{2m}(\\beta_1 ^2 + \\beta_2 ^2)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$p_i = \\frac{1}{1 + e^{-(\\beta_1 x_{i1} + \\beta_2 x_{i2})}}$$\n",
        "\n",
        "For this, you need to create a surface plot which is a three-dimensional graph because you need three axes:\n",
        "\n",
        "1. One axis for plotting the $\\beta_1$ values\n",
        "\n",
        "2. Another axis for plotting the $\\beta_2$ values\n",
        "\n",
        "3. One more axis for plotting the $J$ values\n",
        "\n",
        "To create a three-dimensional plot, you need to\n",
        "\n",
        "- Import the  `Axes3D`  class from the `mpl_toolkits.mplot3d` module\n",
        "\n",
        "- Create an array containing the values for $x$-axis. In this case, $\\beta_1$ values\n",
        "\n",
        "- Create an array containing the values for $y$-axis. In this case, $\\beta_2$ values\n",
        "\n",
        "- Create an $N$-dimensional array containing the ordered pairs of $x$-$y$ values i.e. $(x, y)$. In this case, $(\\beta_1, \\beta_2)$ ordered pairs. You can do this using the `meshgrid(array1, array2)` function of the `numpy` module where the `array1` and `array2` are the arrays whose values will be used to create ordered pairs.\n",
        "\n",
        "- Finally, you need to use the `plot_surface()` function of the `Axes3D` class of the `mpl_toolkits.mplot3d` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwuiyxPsoBVr"
      },
      "source": [
        "# S1.1: Create a surface plot to visualise the regularised cost function as a function of 'beta1' and 'beta2' only.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYtE8LEOsAP-"
      },
      "source": [
        "As you can see, we have created a three-dimensional plot (surface plot to be specific) for the regularised cost function for 1000 different set of $\\beta_1$ and $\\beta_2$ values. You can see that the cost function has its lowest values at points between $\\beta_1 = -2$ and $\\beta_1 = -1$, $\\beta_1 = 1$ and $\\beta_1 = 2$ and $\\beta_2 = -3$ and $\\beta_2 = 4$.\n",
        "\n",
        "**Note:** The real cost function for the dummy dataset is dependent on all the three betas. So, the above three-dimensional plot does not represent the real cost function. In order to visualise the cost function, we discarded $\\beta_0$ because it is physically impossible to create a four-dimensional plot. You could have also discarded either $\\beta_1$ or $\\beta_2$ instead of $\\beta_0$ to create a three-dimensional plot.\n",
        "\n",
        "In the `for` loop in the above code i.e.:\n",
        "\n",
        "```\n",
        "error1 = target_array.reshape(1, m) * np.log(sigmoid(np.matmul(beta[i], np.transpose(features_array))))\n",
        "error2 = (1 - target_array.reshape(1, m)) * np.log(1 - sigmoid(np.matmul(beta[i], np.transpose(features_array))))\n",
        "cost = - (1 / m) * np.sum(error1 + error2) + 10 / (2 * m) * np.sum(beta[i] ** 2)\n",
        "cost_func_values.append(cost)\n",
        "```\n",
        "\n",
        "- The `np.matmul(beta[i], np.transpose(features_array))` multiplies the $B = \\begin{bmatrix} \\beta_1 & \\beta_2\\end{bmatrix}$ matrix with the $X = \\begin{bmatrix}x_{11} & x_{12} \\\\ x_{21} & x_{22} \\\\ x_{31} & x_{32} \\\\ \\vdots & \\vdots \\\\ x_{10001} & x_{10002} \\end{bmatrix}$ matrix i.e. $BX^T$\n",
        "\n",
        "- The `sigmoid(np.matmul(beta[i], np.transpose(features_array)))` part calculates the probabilities i.e. $p = \\frac{1}{1 + e^{-BX^T}}$for one set of $\\beta$ values\n",
        "\n",
        "- The `error1` variable stores the $y_i \\log p_i$ part of the cost function for one set of $\\beta$ values\n",
        "\n",
        "- The `error2` variable stores the $(1 - y_i) \\log (1 - p_i)$ part of the cost function one set of $\\beta$ values\n",
        "\n",
        "- The `cost` variable stores the regularised cost function $-\\frac{1}{m} \\sum_{i = 1}^m \\{ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\} + \\frac{\\lambda}{2m}(\\beta_1 ^2 + \\beta_2 ^2)$ value one set of $\\beta$ values\n",
        "\n",
        "- The `cost_func_values` list stores the regularised cost function values for all the 1000 set of $\\beta$ values.\n",
        "\n",
        "The `np.meshgrid(beta1, beta2)` function creates ordered pairs for the `beta1` and `beta2` array values and stores them in the `meshed_beta1` and `meshed_beta2` variables.\n",
        "\n",
        "The `plt.figure(figsize = (10, 6), dpi = 96)` function creates a `matplotlib` figure object and stores it in the `fig` variable.\n",
        "\n",
        "The `gca(projection = '3d')` function is called on the figure object created above with `projection` as a parameter and its value equal to `'3d'` which asks the `matplotlib.pyplot` object to **get** the **current axis** (full form of GCA). And the parameter, `projection = '3d'`, denotes to get three axes i.e. $x, y, z$ to create a three-dimensional plot. This 3D axis is stored in the `ax` variable.\n",
        "\n",
        "On the 3D axis stored in the `ax` variable, the `plot_surface()` function is called to create a surface plot between $\\beta_1, \\beta_2$ and $J$ for which `meshed_beta1, meshed_beta2, np.array(cost_func_values).reshape(1, m)` inputs are passed to the function. The additional parameter `cmap` (short-form for **colour-map**) defines the colour of the surface plot for which the `cm.coolwarm` value is passed. The `cost_func_values` stores a Python list that is converted to a `numpy` array and having 1 row and 1000 columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwKTKKlRvdL9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecf3CdG8GEkN"
      },
      "source": [
        "#### Activity 2: Rules of Derivatives^^\n",
        "\n",
        "Continuing with the motivation to differentiate the regularised cost function\n",
        "\n",
        "$$J(\\beta) = -\\frac{1}{m} \\sum_{i = 1}^m \\{ y \\log p_i + (1 - y) \\log(1 - p_i) \\} + \\frac{\\lambda}{2m} \\sum_{j = 1}^n \\beta_j^2$$\n",
        "\n",
        "to obtain the optimum beta values, let's quickly go through the rules of derivatives.\n",
        "\n",
        "In the last class, you used the following approach to differentiate $y = \\frac{x^5}{5} - \\frac{26}{3}x^3 + 25x$ wrt $x$\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{dy}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x)}{h}\n",
        "\\end{equation}\n",
        "\n",
        "This is called the first-principle approach. It takes a lot of time to calculate derivatives. Hence, to quicken up the calculation of derivatives, you can use a set of rules of derivatives that are derived using the first principle approach. Eg.\n",
        "\n",
        "To differentiate $y = \\frac{x^5}{5} - \\frac{26}{3}x^3 + 25x$ wrt $x$, you can use the following rule\n",
        "\n",
        "\\begin{align}\n",
        "y &= kx^n \\\\\n",
        "\\frac{dy}{dx} &= k \\frac{d(x^n)}{dx}  \\\\\n",
        "\\Rightarrow \\frac{dy}{dx} &= kn x^{n - 1}\n",
        "\\end{align}\n",
        "\n",
        "which says that the differentiation of a function $y = kx^n$ is $kn$ times $x$ raised to power $n - 1$ where\n",
        "\n",
        "- $k$ is some arbitrary constant and $k \\neq 0$\n",
        "\n",
        "- $n$ is some real number and $n \\neq 0$\n",
        "\n",
        "If $n = 0$, the function becomes $y = k$ and its differentiation wrt any variable $\\frac{dy}{dx} = 0$\n",
        "\n",
        "So, here are the two rules of derivatives that we just discovered.\n",
        "\n",
        "**Rule 1:** For $y = kx^n$\n",
        "\\begin{equation}\n",
        "\\frac{dy}{dx} = kn x^{n - 1}\n",
        "\\end{equation}\n",
        "\n",
        "**Rule 2:** For $y = k$\n",
        "\\begin{equation}\n",
        "\\frac{dy}{dx} = 0\n",
        "\\end{equation}\n",
        "\n",
        "Let's apply the first rule to differentiate the function $y = \\frac{x^5}{5} - \\frac{26}{3}x^3 + 25x$ wrt $x$\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{dy}{dx} &= \\frac{1}{5} \\times \\frac{d(x^5)}{dx} - \\frac{26}{3} \\times \\frac{d(x^3)}{dx} + 25 \\times \\frac{d(x)}{dx} \\\\\n",
        "&= \\frac{1}{5}\\times 5 x^{5 - 1} - \\frac{26}{3} \\times 3 x^{3 - 1} + 25 x^{1 - 1}\\\\\n",
        "&= x^4 - 26x^2 + 25\n",
        "\\end{align}\n",
        "\n",
        "Since we need to differentiate log that is present in the cost function, let's now learn the rule to differentiate a logarithmic function.\n",
        "\n",
        "**Rule 3:** For $y = \\log x$\n",
        "\\begin{equation}\n",
        "\\frac{dy}{dx} = \\frac{1}{x}\n",
        "\\end{equation}\n",
        "\n",
        "If you extend the log function further to $y = \\log (kx^n)$, then its derivative wrt $x$ is\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{dy}{dx} = \\frac{1}{kx^n} \\times \\frac{d(kx^n)}{dx} \\\\\n",
        "\\end{equation}\n",
        "\n",
        "$\\qquad \\qquad \\qquad \\qquad$ [Whatever written in the log goes to the denominator $\\times$ The derivative of whatever written in the log]\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{dy}{dx} &= \\frac{1}{kx^n} \\times kn x^{n - 1} \\\\\n",
        "\\Rightarrow \\frac{dy}{dx} &= \\frac{n}{x}\n",
        "\\end{align}\n",
        "\n",
        "**Rule 4:** For $y =  e^x$\n",
        "\\begin{equation}\n",
        "\\frac{dy}{dx} = e^{x}\n",
        "\\end{equation}\n",
        "\n",
        "If you extend the exponential function further to $y = e^{kx^n}$, then its derivative wrt $x$ is\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{dy}{dx} = e^{kx^n} \\times \\frac{d(kx^n)}{dx} \\\\\n",
        "\\end{equation}\n",
        "\n",
        "$\\qquad \\qquad \\qquad \\qquad$ [$e^{kx^n}$ remains as it is $\\times$ The derivative of whatever written in the power of $e$]\n",
        "\n",
        "\\begin{align}\n",
        "\\Rightarrow \\frac{dy}{dx} &= e^{kx^n} \\times kn x^{n - 1}\n",
        "\\end{align}\n",
        "\n",
        "Using these four rules, finally, we can now differentiate the regularised cost function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brTzahGVGHG5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfCMwGKIiQ4-"
      },
      "source": [
        "#### Activity 3: Derivative of Regularised Cost Function^^^\n",
        "\n",
        "Let's first differentiate the cost function wrt to $\\beta_0$ and simultaneously, we will treat $\\beta_1, \\beta_2, \\beta_3, \\dots, \\beta_n$ as constants i.e as $y = k$ whose derivative will be 0. This is called partial derivatives.\n",
        "\n",
        "**Note:** At this point, you don't need to know the meaning of partial derivatives. Just treat one of the betas as an independent variable at a time and all the other betas as constants.\n",
        "\n",
        "In the case of partial derivatives, the $\\partial$ symbol is used instead of the $d$ symbol. It is commonly pronounced as either **del** or **doh**.\n",
        "\n",
        "\\begin{align}\n",
        "J &= - \\frac{1}{m} \\sum_{i = 1} ^{m} (y_i \\log p_i  + (1 - y_i) \\log(1 - p_i)) + \\frac{\\lambda}{2m}(\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_n^2)\\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_0} &=  - \\frac{1}{m} \\sum_{i = 1} ^{m} \\left( y_i \\times \\frac{1}{p_i} \\times \\frac{\\partial p_i}{\\partial \\beta_0} + (1 - y_i) \\times \\frac{-1}{1 - p_i} \\times \\frac{\\partial p_i}{\\partial \\beta_0} \\right) + \\frac{\\lambda}{2m}\\times 0\n",
        "\\end{align}\n",
        "\n",
        "We know that\n",
        "\n",
        "\\begin{align}\n",
        "p_i &= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}} \\\\\n",
        "\\therefore \\frac{\\partial p_i}{\\partial \\beta_0} &= - \\left( \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}} \\right)^2 (0 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)})(-1 + 0) \\\\\n",
        "&= \\frac{e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}{(1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)})^2} \\\\\n",
        "\\end{align}\n",
        "\n",
        "On adding $-1$ and $1$ to the above equation, we get\n",
        "\n",
        "\\begin{align}\n",
        "\\therefore \\frac{\\partial p_i}{\\partial \\beta_0} &= \\frac{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)} - 1}{(1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)})^2} \\\\\n",
        "&= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}} - \\frac{1}{(1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)})^2} \\\\\n",
        "&= p_i - p_i^2 \\\\\n",
        "&= p_i(1 - p_i)\n",
        "\\end{align}\n",
        "\n",
        "On substituting $\\frac{\\partial p_i}{\\partial \\beta_0}$ in the derivative of the cost function wrt $\\beta_0$, we get\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial J}{\\partial \\beta_0} &=  - \\frac{1}{m} \\sum_{i = 1} ^{m} \\left( y_i \\times \\frac{1}{p_i} \\times p_i(1 - p_i) + (1 - y_i) \\times \\frac{-1}{1 - p_i} \\times p_i(1 - p_i) \\right) \\\\\n",
        "&=  - \\frac{1}{m} \\sum_{i = 1} ^{m} \\left( y_i (1 - p_i) - (1 - y_i) p_i\\right) \\\\\n",
        "&=  - \\frac{1}{m} \\sum_{i = 1} ^{m} (y_i  - y_i p_i - p_i + y_i p_i) \\\\\n",
        "\\Rightarrow \\frac{\\partial J}{\\partial \\beta_0} &=  \\frac{1}{m} \\sum_{i = 1} ^{m} (p_i  - y_i)\n",
        "\\end{align}\n",
        "\n",
        "Similarly, if you differentiate $J$ wrt to $\\beta_1$, you will get\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial J}{\\partial \\beta_1} =  \\frac{1}{m} \\sum_{i = 1} ^{m} (p_i  - y_i)x_1 + \\frac{\\lambda}{m} \\beta_1 \\\\\n",
        "\\end{equation}\n",
        "\n",
        "In general, for $\\beta_j = \\beta_1$ to $\\beta_n$, you will get\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial J}{\\partial \\beta_j} =  \\frac{1}{m} \\sum_{i = 1} ^{m} (p_i  - y_i)x_j + \\frac{\\lambda}{m} \\beta_j \\\\\n",
        "\\end{equation}\n",
        "\n",
        "Let's stop here. In the next class, you will learn to implement these derivatives in Python to calculate the optimum values of $\\beta_0, \\beta_1$ and $\\beta_2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0L1YHWwGD7U"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HgVTE_skNvH"
      },
      "source": [
        "### **Project**\n",
        "\n",
        "You can now attempt the **Applied Tech Project 77 - Surface Plot and Partial Derivatives** on your own.\n",
        "\n",
        "\n",
        "**Applied Tech Project 77 - Surface Plot and Partial Derivatives**: https://colab.research.google.com/drive/15_kCFpLr13tnRBKZWhtXrwWULx0_DIdD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQQGtWRZkNLw"
      },
      "source": [
        "---"
      ]
    }
  ]
}