{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalinis07/APT_Class_Copy_Links/blob/MASTER/AT_Lesson_70_Class_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN_oEsXfM_LV"
      },
      "source": [
        "# Lesson 70: Car Price Prediction - RFE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFjOAv8nAhzQ"
      },
      "source": [
        "### Teacher-Student Activities\n",
        "\n",
        "In this class, you will learn the concept of recursive feature elimination that will help you eliminate the redundant features from a dataset. This will reduce your workload of manually inspecting the significance of each feature by calculating its p-value and variance inflation factor again and again.\n",
        "\n",
        "Let's go through the activities covered in the previous class and begin this class from **Activity 1: Highly Correlated Features** section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rqO31eQB5O-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-uhY3ryaj4N"
      },
      "source": [
        "### Problem Statement\n",
        "\n",
        "Build a linear regression model to predict prices of cars based on its technical specifications such as car manufacturer, its engine capacity, fuel efficiency, body-type etc.\n",
        "\n",
        "**Dataset Description:**\n",
        "\n",
        "The dataset contains 205 rows and 26 columns. Each column represents an attribute of a car as described in the table below.\n",
        "\n",
        "|Sr No.|Attribute|Attribute Information|\n",
        "|-|-|-|\n",
        "|1|Car_ID|Unique id of each car (Integer)|\n",
        "|2|Symboling|Assigned insurance risk rating; a value of +3 indicates that the car is risky; -3 suggests that it is probably a safe car (Categorical)|\n",
        "|3|carCompany|Name of car company (Categorical)|\n",
        "|4|fueltype| fuel-type i.e. petrol or diesel (Categorical)|\n",
        "|5|aspiration|Aspiration used in a car (Categorical)|\n",
        "|6|doornumber|Number of doors in a car (Categorical)|\n",
        "|7|carbody|Body-type of a car (Categorical)|\n",
        "|8|drivewheel|Type of drive wheel (Categorical)|\n",
        "|9|enginelocation|Location of car engine (Categorical)|\n",
        "|10|wheelbase|Wheelbase of car (Numeric)|\n",
        "|11|carlength|Length of car (Numeric)|\n",
        "|12|carwidth|Width of car (Numeric)|\n",
        "|13|carheight|Height of car (Numeric)|\n",
        "|14|curbweight|The weight of a car without occupants or baggage (Numeric)|\n",
        "|15|enginetype|Type of engine (Categorical)|\n",
        "|16|cylindernumber|Number of cylinders placed in the car engine (Categorical)||17|enginesize|Capacity of an engine (Numeric)|\n",
        "|18|fuelsystem|Fuel system of a car (Categorical)|\n",
        "|19|boreratio|Bore ratio of car (Numeric)|\n",
        "|20|stroke|Stroke or volume inside the engine (Numeric)|\n",
        "|21|compressionratio|Compression ratio of an engine (Numeric)|\n",
        "|22|horsepower|Power output of an engine (Numeric)|\n",
        "|23|peakrpm|Peak revolutions per minute (Numeric)|\n",
        "|24|citympg|Mileage in city (Numeric)|\n",
        "|25|highwaympg|Mileage on highway (Numeric)|\n",
        "|26|price(Dependent variable)|Price of a car (Numeric)|\n",
        "\n",
        "This data set consists of three types of entities:\n",
        "\n",
        "- the specification of an auto in terms of various characteristics,\n",
        "\n",
        "- its assigned insurance risk rating,\n",
        "\n",
        "- its normalised losses in use as compared to other cars.\n",
        "\n",
        "The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process **symboling**. A value of $+3$ indicates that the auto is risky, $-3$ that it is probably pretty safe.\n",
        "\n",
        "The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality etc.), and represents the average loss per car per year.\n",
        "\n",
        "**Note:** Several of the attributes in the database could be used as a \"class\" attribute.\n",
        "\n",
        "**Dataset source:** https://archive.ics.uci.edu/ml/datasets/Automobile\n",
        "\n",
        "\n",
        "The above dataset consists of data taken from 1985 Ward's Automotive Yearbook. Here's the list of original sources of the data:\n",
        "\n",
        "1. 1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook.\n",
        "\n",
        "2. Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038\n",
        "\n",
        "3. Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZEA9P6hDG28"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keVeYBHNDHh8"
      },
      "source": [
        "#### Importing Modules & Reading Data\n",
        "\n",
        "https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/car-prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f2emlnJM56A"
      },
      "source": [
        "# Import the modules, read the dataset and create a Pandas DataFrame.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Read the dataset\n",
        "cars_df = pd.read_csv(\"https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/car-prices.csv\")\n",
        "\n",
        "\n",
        "# Extract the name of the manufactures from the car names and display the first 25 cars to verify whether names are extracted successfully.\n",
        "car_companies = pd.Series([car.split(\" \")[0] for car in cars_df['CarName']], index = cars_df.index)\n",
        "\n",
        "# Create a new column named 'car_company'. It should store the company names of a the cars.\n",
        "cars_df['car_company'] = car_companies\n",
        "\n",
        "# Replace the misspelled 'car_company' names with their correct names.\n",
        "# volkswagen\n",
        "cars_df.loc[(cars_df['car_company'] == \"vw\") | (cars_df['car_company'] == \"vokswagen\"), 'car_company'] = 'volkswagen'\n",
        "\n",
        "# porsche\n",
        "cars_df.loc[cars_df['car_company'] == \"porcshce\", 'car_company'] = 'porsche'\n",
        "\n",
        "# toyota\n",
        "cars_df.loc[cars_df['car_company'] == \"toyouta\", 'car_company'] = 'toyota'\n",
        "\n",
        "# nissan\n",
        "cars_df.loc[cars_df['car_company'] == \"Nissan\", 'car_company'] = 'nissan'\n",
        "\n",
        "# mazda\n",
        "cars_df.loc[cars_df['car_company'] == \"maxda\", 'car_company'] = 'mazda'\n",
        "\n",
        "# Drop 'CarName' column from the 'cars_df' DataFrame.\n",
        "cars_df.drop(columns= ['CarName'], axis = 1, inplace = True)\n",
        "\n",
        "# Extract all the numeric (float and int type) columns from the dataset.\n",
        "cars_numeric_df = cars_df.select_dtypes(include = ['int64', 'float64'])\n",
        "\n",
        "# Drop the 'car_ID' column from the 'cars_numeric_df' DataFrame.\n",
        "cars_numeric_df.drop(columns = ['car_ID'], axis = 1, inplace = True)\n",
        "\n",
        "# Map the values of the 'doornumber' and 'cylindernumber' columns to their corresponding numeric values.\n",
        "words_dict = {\"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\": 6, \"eight\": 8, \"twelve\": 12}\n",
        "def num_map(series):\n",
        "    return series.map(words_dict)\n",
        "\n",
        "# Applying the function to the two columns\n",
        "cars_df[['cylindernumber', 'doornumber']] = cars_df[['cylindernumber', 'doornumber']].apply(num_map, axis = 1)\n",
        "\n",
        "# Create dummy variables for the 'carbody' columns.\n",
        "car_body_dummies = pd.get_dummies(cars_df['carbody'], dtype = int)\n",
        "\n",
        "# Create dummy variables for the 'carbody' columns with 1 column less.\n",
        "car_body_new_dummies = pd.get_dummies(cars_df['carbody'], drop_first = True, dtype = int)\n",
        "\n",
        "# Create a DataFrame containing all the non-numeric type features.\n",
        "cars_categorical_df = cars_df.select_dtypes(include = ['object'])\n",
        "\n",
        "#Get dummy variables for all the categorical type columns using the dummy coding process.\n",
        "cars_dummies_df = pd.get_dummies(cars_categorical_df, drop_first = True, dtype = int)\n",
        "\n",
        "#  Drop the categorical type columns from the 'cars_df' DataFrame.\n",
        "cars_df.drop(list(cars_categorical_df.columns), axis = 1, inplace = True)\n",
        "\n",
        "# Concatenate the 'cars_df' and 'cars_dummies_df' DataFrames.\n",
        "cars_df = pd.concat([cars_df, cars_dummies_df], axis = 1)\n",
        "\n",
        "#  Drop the 'car_ID' column\n",
        "cars_df.drop('car_ID', axis = 1, inplace = True)\n",
        "\n",
        "# Split the 'cars_df' Dataframe into the train and test sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(cars_df, test_size = 0.3, random_state = 42)\n",
        "\n",
        "# Create separate data-frames for the feature and target variables for both the train and test sets.\n",
        "features = list(cars_df.columns)\n",
        "features.remove('price')\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df['price']\n",
        "X_test = test_df[features]\n",
        "y_test = test_df['price']\n",
        "\n",
        "# Normalise only the numeric columns.\n",
        "def standard_norm(series):\n",
        "  series_mean = series.mean()\n",
        "  series_std = series.std()\n",
        "  new_series = (series - series_mean) / series_std\n",
        "  return new_series\n",
        "\n",
        "X_train[X_train.columns[:16]] = X_train[X_train.columns[:16]].apply(standard_norm, axis = 0)\n",
        "X_test[X_test.columns[:16]] = X_test[X_test.columns[:16]].apply(standard_norm, axis = 0)\n",
        "\n",
        "#  Build a linear regression model using all the features to predict car prices.\n",
        "import statsmodels.api as sm\n",
        "\n",
        "X_train_sm = sm.add_constant(X_train)\n",
        "lin_reg = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "print(lin_reg.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI8qitijEfI3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAG0OV_ylfvx"
      },
      "source": [
        "#### Activity 1: Highly Correlated Features^\n",
        "\n",
        "Let's create a Python dictionary storing the moderately to highly correlated features with price and the corresponding correlation values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6IRcUUV-O0A"
      },
      "source": [
        "# S1.1: Create a Python dictionary storing the moderately to highly correlated features with price and the corresponding correlation values.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiRuz7KQ7SK4"
      },
      "source": [
        "Let's also create a heatmap to visualise the correlation between the above features (if there exists)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzgPP3pj7dRS"
      },
      "source": [
        "# S1.2: Create a heatmap to visualise the correlation between the above features (if there exists).\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICeU3-Sj6A6H"
      },
      "source": [
        "There are a lot of features that are highly correlated. This means there is high probability of multicollinearity.\n",
        "\n",
        "Now, you know that these are the features that affect the price of a car the most. Hence, you should choose the best ones among these features such that they significantly contribute in predicting the price of a car and there is negligible multicollinearity between them. For this, you can use recursive feature elimination technique.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjRsCx_43rbl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3n9WzqImdBs"
      },
      "source": [
        "#### Activity 2: Recursive Feature Elimination (RFE)^^\n",
        "\n",
        "Recursive feature elimination (RFE) is a feature selection (or elimination) method that fits a model and removes the weakest feature (or features). Here, you need to decide the numbers of features you want to select to build a model. Then you can validate your choice of number of features and increase or decrease them (if required). Features are ranked by the model's `coef_` or `feature_importances_` attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in a machine learning model.\n",
        "\n",
        "RFE requires a specified number of features to keep, however it is often not known in advance how many features are valid.\n",
        "\n",
        "Let's understand this concept with the help of an example. Let's select the best 10 features out of the highly correlated features that you just stored in the dictionary above.\n",
        "\n",
        "To use RFE, you need to\n",
        "\n",
        "1. Import the `RFE` class from the `sklearn.feature_selection` module.\n",
        "\n",
        "2. Create an object of a `LinearRegression` class. Let's name it `skl_lin_reg`.\n",
        "\n",
        "3. Create an object of the `RFE` class. Let's name it `rfe1`. The constructor of the `RFE` object takes the object of `LinearRegression` class (i.e. `skl_lin_reg`) and the number of features to be selected using RFE as inputs.\n",
        "\n",
        "4. Call the `fit()` function on the `RFE` object with the train set (separate sets for independent variables and dependent variable) as input to the function.\n",
        "\n",
        "Use the `support_` attribute of the `RFE` class to get an array containing boolean values wherein `True` denotes the feature selected by RFE. You can also use the `ranking_` attribute of the `RFE` class to to get the rankings of the features. The features selected by `RFE` are ranked $1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyKg96LKAvxr"
      },
      "source": [
        "# T2.1: Use RFE to eliminate few features from the dataset.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCl0OnEVJPL9"
      },
      "source": [
        "As you can see, the features having the corresponding `True` or `1` values are the ones selected by RFE.\n",
        "\n",
        "Let's print these features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8wpwggkKG5b"
      },
      "source": [
        "# T2.2: Print the 10 features selected by RFE in the previous step.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKc1kPpKFgy"
      },
      "source": [
        "Now, let's build a linear regression model using the `statsmodels.api` module and validate the significance (by calculating the p-values and VIF values) of the 10 features selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvYP0SwKBAQE"
      },
      "source": [
        "# T2.3: Build a linear regression model using the 'statsmodels.api' module having the above 10 features selected using RFE.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVpyMD_tLskx"
      },
      "source": [
        "The $R^2$ and adjusted $R^2$ values are close to $1$ which is a good sign. However, the p-values for a few features are greater than 0.05 which is not a good sign. So let's also calculate the VIF values before you further eliminate more features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWYhQwLNBoyP"
      },
      "source": [
        "# T2.4: Check for the VIF values of the 10 features selected by RFE above.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AauTo3Qp9Tqs"
      },
      "source": [
        "Here, you can see that a couple features have high VIF values which means there's a high probability of multicollinearity even though the `statsmodels.api` didn't throw a multicollinearity warning message. You need to remove this multicollinearity either by one-by-one eliminating features or by selecting the fewer features. Let's go with the latter approach.\n",
        "\n",
        "\n",
        "**Note:** As a guideline, to further eliminate the features:\n",
        "\n",
        "1. Calculate the number of features having high p-values. Let's say this is 5.\n",
        "\n",
        "2. Calculate the number of features having high VIF values. Let's say this is 3.\n",
        "\n",
        "3. The number of features to be further eliminated would be the maximum of the numbers obtained in steps 1 and 2. In this case, $\\text{max}\\{5, 3\\} = 5$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyCa-EZ9919L"
      },
      "source": [
        "# S2.1: Use RFE to select only 5 features to predict the price of a car.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ER2mptiOsNx"
      },
      "source": [
        "# S2.2: Print the 5 features selected by RFE in the previous step.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USJ2eO1zOx-6"
      },
      "source": [
        "Now build the linear regression model again using the most recently selected features by RFE. Use the `statsmodels.api` module to build the linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_usSx1f6-GHm"
      },
      "source": [
        "# S2.3: Build the linear regression model again using the most recently selected features by RFE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Hw7uzfPasE"
      },
      "source": [
        "As you can see, the $R^2$ and adjusted $R^2$ values are pretty high. The p-values for all the 5 features selected are less than 0.05. These are great signs. Now let's calculate the VIF values for these 5 features to see if there is negligible multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8jJLtoq-UBY"
      },
      "source": [
        "# S2.4: Check for the VIF values of the feature variables.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKCxYZgg-xKz"
      },
      "source": [
        "As you can see, the VIF values for all the features is less than 10. Hence, we can safely assume that the multicollinearity between these features is negligible.\n",
        "\n",
        "Let's stop here. In the next class, we will evaluate this linear regression model further using the MAE, MSE, RMSE parameters and also analyse the residuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6a2op5DP9jv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATmy7EDjtFgW"
      },
      "source": [
        "### **Project**\n",
        "You can now attempt the **Applied Tech. Project 70 - Car Price Prediction - RFE** on your own.\n",
        "\n",
        "**Applied Tech. Project 70 - Car Price Prediction - RFE**: https://colab.research.google.com/drive/1652FnxXEMT1dfXwJ5jVOrY61xG3CO_BG?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i9vyKvStNnJ"
      },
      "source": [
        "---"
      ]
    }
  ]
}